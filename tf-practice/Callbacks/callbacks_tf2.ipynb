{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the diabetes dataset\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "diabetes_dataset = load_diabetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the input and target variables\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = diabetes_dataset['data']\n",
    "targets = diabetes_dataset['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data set into training and test sets\n",
    "\n",
    "train_data, test_data, train_targets, test_targets = train_test_split(data, targets, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(train_data.shape[1],)),\n",
    "    Dense(64,activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1)        \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "    \n",
    "model.compile(loss='mse', optimizer=\"adam\", metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a Custom Callback\n",
    "\n",
    "We define our custom callback using the logs dictionary to access the loss and metric values.\n",
    "The logs dictionary stores the loss value, along with all of the metrics we are using at the end of a batch or epoch.\n",
    "\n",
    "We can incorporate information from the logs dictionary into our own custom callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossAndMetricCallback(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    # Print the loss after every second batch in the training set\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        if batch%2 == 0:\n",
    "            print('\\n After batch {}, the loss is {:7.2f}'.format(batch, logs['loss']))\n",
    "    \n",
    "    # Print the loss after each batch in the test set\n",
    "    def on_test_batch_end(self, batch, logs=None):\n",
    "        print('\\n After batch {}, the loss is {:7.2f}'.format(batch, logs['loss']))\n",
    "    \n",
    "    # Print the loss and mae after each epoch in the training set\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print('\\n Epoch {}: Average Loss is {:7.2f}, mean absolute error is {:7.2f}'.format(epoch, logs['loss']\n",
    "                                                                                            , logs['mae']))\n",
    "    # Notify the user when prediction has finished after each batch\n",
    "    def on_predict_batch_end(self, batch, logs=None):\n",
    "        print('Finished prediction on batch {}'.format(batch))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " After batch 0, the loss is 40133.84\n",
      "\n",
      " After batch 2, the loss is 30823.12\n",
      "\n",
      " After batch 4, the loss is 34196.52\n",
      "\n",
      " After batch 6, the loss is 31098.02\n",
      "\n",
      " After batch 8, the loss is 31281.37\n",
      "\n",
      " After batch 10, the loss is 25813.80\n",
      "\n",
      " After batch 12, the loss is 25389.82\n",
      "\n",
      " Epoch 0: Average Loss is 29681.41, mean absolute error is  153.87\n",
      "\n",
      " After batch 0, the loss is 34105.68\n",
      "\n",
      " After batch 2, the loss is 27500.86\n",
      "\n",
      " After batch 4, the loss is 25390.81\n",
      "\n",
      " After batch 6, the loss is 32050.20\n",
      "\n",
      " After batch 8, the loss is 25587.55\n",
      "\n",
      " After batch 10, the loss is 22701.82\n",
      "\n",
      " After batch 12, the loss is 33020.29\n",
      "\n",
      " Epoch 1: Average Loss is 28615.63, mean absolute error is  150.74\n",
      "\n",
      " After batch 0, the loss is 31808.57\n",
      "\n",
      " After batch 2, the loss is 21785.64\n",
      "\n",
      " After batch 4, the loss is 18827.32\n",
      "\n",
      " After batch 6, the loss is 26694.63\n",
      "\n",
      " After batch 8, the loss is 20967.12\n",
      "\n",
      " After batch 10, the loss is 22416.49\n",
      "\n",
      " After batch 12, the loss is 35211.75\n",
      "\n",
      " Epoch 2: Average Loss is 25598.78, mean absolute error is  141.57\n",
      "\n",
      " After batch 0, the loss is 18850.58\n",
      "\n",
      " After batch 2, the loss is 24250.96\n",
      "\n",
      " After batch 4, the loss is 16327.90\n",
      "\n",
      " After batch 6, the loss is 19263.38\n",
      "\n",
      " After batch 8, the loss is 18420.66\n",
      "\n",
      " After batch 10, the loss is 14747.70\n",
      "\n",
      " After batch 12, the loss is 11497.16\n",
      "\n",
      " Epoch 3: Average Loss is 19854.33, mean absolute error is  122.01\n",
      "\n",
      " After batch 0, the loss is 14959.46\n",
      "\n",
      " After batch 2, the loss is 18539.42\n",
      "\n",
      " After batch 4, the loss is 14613.27\n",
      "\n",
      " After batch 6, the loss is 12518.00\n",
      "\n",
      " After batch 8, the loss is 11475.46\n",
      "\n",
      " After batch 10, the loss is 8247.87\n",
      "\n",
      " After batch 12, the loss is 7235.50\n",
      "\n",
      " Epoch 4: Average Loss is 12496.72, mean absolute error is   91.45\n",
      "\n",
      " After batch 0, the loss is 7227.70\n",
      "\n",
      " After batch 2, the loss is 6410.85\n",
      "\n",
      " After batch 4, the loss is 6698.16\n",
      "\n",
      " After batch 6, the loss is 5854.45\n",
      "\n",
      " After batch 8, the loss is 9648.60\n",
      "\n",
      " After batch 10, the loss is 5878.26\n",
      "\n",
      " After batch 12, the loss is 3925.47\n",
      "\n",
      " Epoch 5: Average Loss is 7415.98, mean absolute error is   65.56\n",
      "\n",
      " After batch 0, the loss is 7790.11\n",
      "\n",
      " After batch 2, the loss is 6623.98\n",
      "\n",
      " After batch 4, the loss is 6347.38\n",
      "\n",
      " After batch 6, the loss is 3822.93\n",
      "\n",
      " After batch 8, the loss is 7318.23\n",
      "\n",
      " After batch 10, the loss is 9116.40\n",
      "\n",
      " After batch 12, the loss is 2928.17\n",
      "\n",
      " Epoch 6: Average Loss is 6303.10, mean absolute error is   64.22\n",
      "\n",
      " After batch 0, the loss is 4050.21\n",
      "\n",
      " After batch 2, the loss is 3545.94\n",
      "\n",
      " After batch 4, the loss is 8183.58\n",
      "\n",
      " After batch 6, the loss is 3514.82\n",
      "\n",
      " After batch 8, the loss is 4290.06\n",
      "\n",
      " After batch 10, the loss is 6612.51\n",
      "\n",
      " After batch 12, the loss is 2881.02\n",
      "\n",
      " Epoch 7: Average Loss is 5591.97, mean absolute error is   59.91\n",
      "\n",
      " After batch 0, the loss is 6758.27\n",
      "\n",
      " After batch 2, the loss is 5110.06\n",
      "\n",
      " After batch 4, the loss is 9138.98\n",
      "\n",
      " After batch 6, the loss is 1362.47\n",
      "\n",
      " After batch 8, the loss is 5419.78\n",
      "\n",
      " After batch 10, the loss is 6627.78\n",
      "\n",
      " After batch 12, the loss is 5899.77\n",
      "\n",
      " Epoch 8: Average Loss is 5047.64, mean absolute error is   55.59\n",
      "\n",
      " After batch 0, the loss is 5883.16\n",
      "\n",
      " After batch 2, the loss is 2459.00\n",
      "\n",
      " After batch 4, the loss is 4000.52\n",
      "\n",
      " After batch 6, the loss is 2027.95\n",
      "\n",
      " After batch 8, the loss is 3949.27\n",
      "\n",
      " After batch 10, the loss is 3214.68\n",
      "\n",
      " After batch 12, the loss is 3598.22\n",
      "\n",
      " Epoch 9: Average Loss is 4371.65, mean absolute error is   52.52\n",
      "\n",
      " After batch 0, the loss is 3260.90\n",
      "\n",
      " After batch 2, the loss is 3522.55\n",
      "\n",
      " After batch 4, the loss is 5834.27\n",
      "\n",
      " After batch 6, the loss is 4178.39\n",
      "\n",
      " After batch 8, the loss is 3667.67\n",
      "\n",
      " After batch 10, the loss is 3389.70\n",
      "\n",
      " After batch 12, the loss is 2042.45\n",
      "\n",
      " Epoch 10: Average Loss is 3885.04, mean absolute error is   49.44\n",
      "\n",
      " After batch 0, the loss is 4783.86\n",
      "\n",
      " After batch 2, the loss is 4249.64\n",
      "\n",
      " After batch 4, the loss is 1749.17\n",
      "\n",
      " After batch 6, the loss is 3986.61\n",
      "\n",
      " After batch 8, the loss is 3052.04\n",
      "\n",
      " After batch 10, the loss is 3409.57\n",
      "\n",
      " After batch 12, the loss is 6008.52\n",
      "\n",
      " Epoch 11: Average Loss is 4346.64, mean absolute error is   52.19\n",
      "\n",
      " After batch 0, the loss is 2483.68\n",
      "\n",
      " After batch 2, the loss is 3823.84\n",
      "\n",
      " After batch 4, the loss is 3856.83\n",
      "\n",
      " After batch 6, the loss is 3919.82\n",
      "\n",
      " After batch 8, the loss is 4159.53\n",
      "\n",
      " After batch 10, the loss is 3185.82\n",
      "\n",
      " After batch 12, the loss is 5814.84\n",
      "\n",
      " Epoch 12: Average Loss is 3978.60, mean absolute error is   50.10\n",
      "\n",
      " After batch 0, the loss is 4937.64\n",
      "\n",
      " After batch 2, the loss is 3295.26\n",
      "\n",
      " After batch 4, the loss is 2990.13\n",
      "\n",
      " After batch 6, the loss is 3637.90\n",
      "\n",
      " After batch 8, the loss is 3914.92\n",
      "\n",
      " After batch 10, the loss is 2951.34\n",
      "\n",
      " After batch 12, the loss is 2965.01\n",
      "\n",
      " Epoch 13: Average Loss is 3269.53, mean absolute error is   46.02\n",
      "\n",
      " After batch 0, the loss is 2331.95\n",
      "\n",
      " After batch 2, the loss is 2959.20\n",
      "\n",
      " After batch 4, the loss is 2885.84\n",
      "\n",
      " After batch 6, the loss is 2539.92\n",
      "\n",
      " After batch 8, the loss is 1907.23\n",
      "\n",
      " After batch 10, the loss is 5255.48\n",
      "\n",
      " After batch 12, the loss is 2993.17\n",
      "\n",
      " Epoch 14: Average Loss is 3342.71, mean absolute error is   47.40\n",
      "\n",
      " After batch 0, the loss is 3007.28\n",
      "\n",
      " After batch 2, the loss is 2356.29\n",
      "\n",
      " After batch 4, the loss is 3634.20\n",
      "\n",
      " After batch 6, the loss is 3126.75\n",
      "\n",
      " After batch 8, the loss is 2858.80\n",
      "\n",
      " After batch 10, the loss is 3406.29\n",
      "\n",
      " After batch 12, the loss is 3553.36\n",
      "\n",
      " Epoch 15: Average Loss is 3364.84, mean absolute error is   47.43\n",
      "\n",
      " After batch 0, the loss is 3775.70\n",
      "\n",
      " After batch 2, the loss is 2128.35\n",
      "\n",
      " After batch 4, the loss is 4045.64\n",
      "\n",
      " After batch 6, the loss is 1570.32\n",
      "\n",
      " After batch 8, the loss is 3999.04\n",
      "\n",
      " After batch 10, the loss is 2503.00\n",
      "\n",
      " After batch 12, the loss is 4174.89\n",
      "\n",
      " Epoch 16: Average Loss is 3079.60, mean absolute error is   44.16\n",
      "\n",
      " After batch 0, the loss is 2210.96\n",
      "\n",
      " After batch 2, the loss is 1704.67\n",
      "\n",
      " After batch 4, the loss is 3684.80\n",
      "\n",
      " After batch 6, the loss is 3188.97\n",
      "\n",
      " After batch 8, the loss is 2414.53\n",
      "\n",
      " After batch 10, the loss is 2779.71\n",
      "\n",
      " After batch 12, the loss is 1860.66\n",
      "\n",
      " Epoch 17: Average Loss is 3170.11, mean absolute error is   45.31\n",
      "\n",
      " After batch 0, the loss is 3914.16\n",
      "\n",
      " After batch 2, the loss is 1877.45\n",
      "\n",
      " After batch 4, the loss is 1976.75\n",
      "\n",
      " After batch 6, the loss is 2965.02\n",
      "\n",
      " After batch 8, the loss is 3274.99\n",
      "\n",
      " After batch 10, the loss is 3339.38\n",
      "\n",
      " After batch 12, the loss is 2094.38\n",
      "\n",
      " Epoch 18: Average Loss is 3004.34, mean absolute error is   44.88\n",
      "\n",
      " After batch 0, the loss is 2686.09\n",
      "\n",
      " After batch 2, the loss is 4186.67\n",
      "\n",
      " After batch 4, the loss is 3824.16\n",
      "\n",
      " After batch 6, the loss is 3583.49\n",
      "\n",
      " After batch 8, the loss is 2389.02\n",
      "\n",
      " After batch 10, the loss is 4344.10\n",
      "\n",
      " After batch 12, the loss is 4366.31\n",
      "\n",
      " Epoch 19: Average Loss is 3623.91, mean absolute error is   48.26\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "history = model.fit(train_data, train_targets, epochs=20,\n",
    "                    batch_size=32, callbacks=[LossAndMetricCallback()], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " After batch 0, the loss is 11797.54\n",
      "\n",
      " After batch 1, the loss is 12818.25\n",
      "\n",
      " After batch 2, the loss is 10156.40\n",
      "\n",
      " After batch 3, the loss is 2374.30\n",
      "\n",
      " After batch 4, the loss is 1558.41\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "model_eval = model.evaluate(test_data, test_targets, batch_size=10, \n",
    "                            callbacks=[LossAndMetricCallback()], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished prediction on batch 0\n",
      "Finished prediction on batch 1\n",
      "Finished prediction on batch 2\n",
      "Finished prediction on batch 3\n",
      "Finished prediction on batch 4\n"
     ]
    }
   ],
   "source": [
    "# Get predictions from the model\n",
    "\n",
    "model_pred = model.predict(test_data, batch_size=10,\n",
    "                           callbacks=[LossAndMetricCallback()], verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Scheduler:\n",
    "\n",
    "We are going to define a callback to change the learning rate of the optimiser of a model during training. We will do this by specifying the epochs and new learning rates where we would like it to be changed.\n",
    "\n",
    "First we define the auxillary function that returns the learning rate for each epoch based on our schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the learning rate schedule. The tuples below are (start_epoch, new_lr)\n",
    "\n",
    "\n",
    "lr_schedule = [\n",
    "    (4, 0.03), (7, 0.02), (11, 0.005), (15, 0.007)\n",
    "]\n",
    "\n",
    "def get_new_epoch_lr(epoch, lr):\n",
    "    # Checks to see if the new input epoch is listed in the learning rate schedule and if so, \n",
    "    # returns learning rate lr_schedule\n",
    "    epoch_in_sched = [i for i in range(len(lr_schedule)) if lr_schedule[i][0] == int(epoch)]\n",
    "    \n",
    "    if len(epoch_in_sched)>0:\n",
    "        # if the epoch exists in the lr_schedule, we return the corresponding learning rate\n",
    "        return lr_schedule[epoch_in_sched[0]][1]\n",
    "    else:\n",
    "        # return the existing learning rate\n",
    "        return lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the custom callback\n",
    "\n",
    "class LRScheduler(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, new_lr):\n",
    "        super(LRScheduler, self).__init__()\n",
    "        # Add the new learning rate function to our callback\n",
    "        self.new_lr = new_lr\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        # Make sure that the optimizer we have chosen has a learning rate, raise an error if not\n",
    "        if not hasattr(self.model.optimizer, 'lr'):\n",
    "            raise ValueError('Error: Optimizer does not have a learning rate')\n",
    "        \n",
    "        # Get the current learning rate\n",
    "        curr_rate = float(tf.keras.backend.get_value(self.model.optimizer.lr))\n",
    "        \n",
    "        # Call the auxilliary function to get the scheduled learning rate for the current epoch\n",
    "        scheduled_rate = self.new_lr(epoch, curr_rate)\n",
    "        \n",
    "        # Set the learning rate to the new scheduled learning rate\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_rate)\n",
    "        print('Learning rate for epoch {} is {:7.3f}'.format(epoch, scheduled_rate))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the same model as before\n",
    "\n",
    "new_model = tf.keras.Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(train_data.shape[1],)),\n",
    "    Dense(64,activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1)        \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "\n",
    "new_model.compile(loss='mse',\n",
    "                optimizer=\"adam\",\n",
    "                metrics=['mae', 'mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate for epoch 0 is   0.001\n",
      "Learning rate for epoch 1 is   0.001\n",
      "Learning rate for epoch 2 is   0.001\n",
      "Learning rate for epoch 3 is   0.001\n",
      "Learning rate for epoch 4 is   0.030\n",
      "Learning rate for epoch 5 is   0.030\n",
      "Learning rate for epoch 6 is   0.030\n",
      "Learning rate for epoch 7 is   0.020\n",
      "Learning rate for epoch 8 is   0.020\n",
      "Learning rate for epoch 9 is   0.020\n",
      "Learning rate for epoch 10 is   0.020\n",
      "Learning rate for epoch 11 is   0.005\n",
      "Learning rate for epoch 12 is   0.005\n",
      "Learning rate for epoch 13 is   0.005\n",
      "Learning rate for epoch 14 is   0.005\n",
      "Learning rate for epoch 15 is   0.007\n",
      "Learning rate for epoch 16 is   0.007\n",
      "Learning rate for epoch 17 is   0.007\n",
      "Learning rate for epoch 18 is   0.007\n",
      "Learning rate for epoch 19 is   0.007\n"
     ]
    }
   ],
   "source": [
    "# Fit the model with our learning rate scheduler callback\n",
    "\n",
    "new_history = new_model.fit(train_data, train_targets, epochs=20,\n",
    "                            batch_size=100, callbacks=[LRScheduler(get_new_epoch_lr)], verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built-in Learning Rate Scheduler:\n",
    "\n",
    "**Usage:** `tf.keras.callbacks.LearningRateScheduler(schedule, verbose=0)`\n",
    "\n",
    "As in our custom callback, the `LearningRateScheduler` in Keras takes a function `schedule` as an argument. \n",
    "\n",
    "This function `schedule` should take two arguments:\n",
    "* The current epoch (as an integer), and\n",
    "* The current learning rate,\n",
    "\n",
    "and return new learning rate for that epoch. \n",
    "\n",
    "The `LearningRateScheduler` also has an optional `verbose` argument, which prints information about the learning rate if it is set to 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the learning rate schedule function:\n",
    "\n",
    "def new_lr(epoch, lr):\n",
    "    if epoch%2 == 0:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr + epoch/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0020000000474974513.\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0020000000949949026.\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.005000000094994903.\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.009999999888241292.\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.01699999977648258.\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.016999999061226845.\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.025999999061226846.\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "history1 = model.fit(train_data, train_targets, epochs=10,\n",
    "                    callbacks=[tf.keras.callbacks.LearningRateScheduler(new_lr, verbose=1)], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.3333333333333333.\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.125.\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.07692307692307693.\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.05555555555555555.\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.043478260869565216.\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.03571428571428571.\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.030303030303030304.\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.02631578947368421.\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.023255813953488372.\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.020833333333333332.\n"
     ]
    }
   ],
   "source": [
    "# We can also use lambda functions to schedule lr\n",
    "\n",
    "history2 = model.fit(train_data, train_targets, epochs=10,\n",
    "                    callbacks=[tf.keras.callbacks.LearningRateScheduler(lambda x:1/(3+5*x), verbose=1)], \n",
    "                    verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV logger\n",
    "\n",
    "**Usage** `tf.keras.callbacks.CSVLogger(filename, separator=',', append=False)`\n",
    "\n",
    "This callback streams the results from each epoch into a CSV file.\n",
    "The first line of the CSV file will be the names of pieces of information recorded on each subsequent line, beginning with the epoch and loss value. The values of metrics at the end of each epoch will also be recorded.\n",
    "\n",
    "The only compulsory argument is the `filename` for the log to be streamed to. This could also be a filepath.\n",
    "\n",
    "You can also specify the `separator` to be used between entries on each line.\n",
    "\n",
    "The `append` argument allows you the option to append your results to an existing file with the same name. This can be particularly useful if you are continuing training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with a CSV logger\n",
    "\n",
    "history3 = model.fit(train_data, train_targets, epochs=10,\n",
    "                    callbacks=[tf.keras.callbacks.CSVLogger(\"results.csv\")], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>mae</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24999.678443</td>\n",
       "      <td>137.74657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24903.073663</td>\n",
       "      <td>137.40007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24808.604750</td>\n",
       "      <td>137.05772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24716.943138</td>\n",
       "      <td>136.71483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24623.719016</td>\n",
       "      <td>136.37712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24532.615928</td>\n",
       "      <td>136.04553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>24443.676239</td>\n",
       "      <td>135.71327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>24355.196073</td>\n",
       "      <td>135.38320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>24266.725943</td>\n",
       "      <td>135.06126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>24180.958099</td>\n",
       "      <td>134.74161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               loss        mae\n",
       "epoch                         \n",
       "0      24999.678443  137.74657\n",
       "1      24903.073663  137.40007\n",
       "2      24808.604750  137.05772\n",
       "3      24716.943138  136.71483\n",
       "4      24623.719016  136.37712\n",
       "5      24532.615928  136.04553\n",
       "6      24443.676239  135.71327\n",
       "7      24355.196073  135.38320\n",
       "8      24266.725943  135.06126\n",
       "9      24180.958099  134.74161"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the CSV\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.read_csv(\"results.csv\", index_col='epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lambda callbacks\n",
    "\n",
    "**Usage** `tf.keras.callbacks.LambdaCallback(\n",
    "        on_epoch_begin=None, on_epoch_end=None, \n",
    "        on_batch_begin=None, on_batch_end=None, \n",
    "        on_train_begin=None, on_train_end=None)`\n",
    "\n",
    "Lambda callbacks are used to quickly define simple custom callbacks with the use of lambda functions.\n",
    "\n",
    "Each of the functions require some positional arguments.\n",
    "* `on_epoch_begin` and `on_epoch_end` expect two arguments: `epoch` and `logs`,\n",
    "* `on_batch_begin` and `on_batch_end` expect two arguments: `batch` and `logs` and\n",
    "* `on_train_begin` and `on_train_end` expect one argument: `logs`.\n",
    "\n",
    "Let's see an example of this in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the epoch number at the beginning of each epoch\n",
    "\n",
    "epoch_callback = tf.keras.callbacks.LambdaCallback(\n",
    "on_epoch_begin=lambda epoch, logs: print('Starting epoch {}'.format(epoch+1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the loss at the end of each batch\n",
    "\n",
    "batch_loss_callback = tf.keras.callbacks.LambdaCallback(\n",
    "    on_batch_end=lambda batch,logs: print('\\n After batch {}, the loss is {:7.2f}.'.format(batch, logs['loss'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inform that training is finished\n",
    "\n",
    "train_finish_callback = tf.keras.callbacks.LambdaCallback(\n",
    "    on_train_end=lambda logs: print('Training finished!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "\n",
      " After batch 0, the loss is 22544.61.\n",
      "\n",
      " After batch 1, the loss is 26039.82.\n",
      "\n",
      " After batch 2, the loss is 23047.82.\n",
      "\n",
      " After batch 3, the loss is 24878.44.\n",
      "Starting epoch 2\n",
      "\n",
      " After batch 0, the loss is 26565.73.\n",
      "\n",
      " After batch 1, the loss is 24514.89.\n",
      "\n",
      " After batch 2, the loss is 22227.91.\n",
      "\n",
      " After batch 3, the loss is 23044.15.\n",
      "Starting epoch 3\n",
      "\n",
      " After batch 0, the loss is 22309.18.\n",
      "\n",
      " After batch 1, the loss is 24282.60.\n",
      "\n",
      " After batch 2, the loss is 24771.15.\n",
      "\n",
      " After batch 3, the loss is 24940.61.\n",
      "Starting epoch 4\n",
      "\n",
      " After batch 0, the loss is 24256.11.\n",
      "\n",
      " After batch 1, the loss is 21205.49.\n",
      "\n",
      " After batch 2, the loss is 24652.61.\n",
      "\n",
      " After batch 3, the loss is 26120.96.\n",
      "Starting epoch 5\n",
      "\n",
      " After batch 0, the loss is 25912.91.\n",
      "\n",
      " After batch 1, the loss is 19946.36.\n",
      "\n",
      " After batch 2, the loss is 24268.54.\n",
      "\n",
      " After batch 3, the loss is 26000.64.\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "# Train the model with the lambda callbacks\n",
    "\n",
    "history4 = model.fit(train_data, train_targets, epochs=5, batch_size=100,\n",
    "                    callbacks=[epoch_callback, batch_loss_callback,train_finish_callback], verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce learning rate on plateau\n",
    "\n",
    "**Usage** `tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss', \n",
    "            factor=0.1, \n",
    "            patience=10, \n",
    "            verbose=0, \n",
    "            mode='auto', \n",
    "            min_delta=0.0001, \n",
    "            cooldown=0, \n",
    "            min_lr=0)`\n",
    "\n",
    "The `ReduceLROnPlateau` callback allows reduction of the learning rate when a metric has stopped improving. \n",
    "The arguments are similar to those used in the `EarlyStopping` callback.\n",
    "* The argument `monitor` is used to specify which metric to base the callback on.\n",
    "* The `factor` is the factor by which the learning rate decreases i.e., new_lr=factor*old_lr.\n",
    "* The `patience` is the number of epochs where there is no improvement on the monitored metric before the learning rate is reduced.\n",
    "* The `verbose` argument will produce progress messages when set to 1.\n",
    "* The `mode` determines whether the learning rate will decrease when the monitored quantity stops increasing (`max`) or decreasing (`min`). The `auto` setting causes the callback to infer the mode from the monitored quantity.\n",
    "* The `min_delta` is the smallest change in the monitored quantity to be deemed an improvement.\n",
    "* The `cooldown` is the number of epochs to wait after the learning rate is changed before the callback resumes normal operation.\n",
    "* The `min_lr` is a lower bound on the learning rate that the callback will produce.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with the ReduceLROnPlateau callback\n",
    "\n",
    "history5 = model.fit(train_data, train_targets, epochs=100, batch_size=100,\n",
    "                    callbacks=[tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                        monitor=\"loss\",factor=0.2, verbose=1)], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
