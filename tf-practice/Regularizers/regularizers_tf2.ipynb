{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularizers:\n",
    "\n",
    "### TL;DR:\n",
    "\n",
    "You have the regression equation $y = Wx+b$, where $x$ is the input, $W$ the weights matrix and $b$ the bias.\n",
    "\n",
    " - Kernel Regularizer: Tries to reduce the weights $W$ (excluding bias).\n",
    " - Bias Regularizer: Tries to reduce the bias $b$.\n",
    " - Activity Regularizer: Tries to reduce the layer's output $y$, thus will reduce the\n",
    "   weights and adjust bias so $Wx+b$ is smallest.\n",
    "\n",
    "Usually if you have no prior on the distribution that you wish to model, you would only use the Kernel Regularizer, since a large enough network can still model your function even if the regularization on the weights are big.\n",
    "\n",
    "If you want the output function to pass through (or have an intercept closer to) the origin, you can use the Bias Regularizer.  \n",
    "If you want the output to be smaller (or closer to 0), you can use the Activity Regularizer.\n",
    "\n",
    "\n",
    "Now for the $L1$ versus $L2$ loss for **weight decay** (not to be confused with the outputs loss function).  \n",
    "$L2$ loss is defined as $w^2$  \n",
    "$L1$ loss is defined as $|w|$.  \n",
    "$w$ is a component of the matrix $W$.\n",
    "\n",
    "The gradient of $L2$ will be: $2w$  \n",
    "The gradient of $L1$ will be: $sign(w)$\n",
    "\n",
    "Thus, for each gradient update with a learning rate $a$, in $L2$ loss, the weights will be subtracted by $aW$, while in $L1$ loss they will be subtracted by $a \\cdot sign(W)$.\n",
    "\n",
    "The effect of $L2$ loss on the weights is a reduction of large components in the matrix $W$ while $L1$ loss will make the weights matrix sparse, with many zero values. The same applies on the bias and output respectively using the bias and activity regularizer.\n",
    "\n",
    "#### NOTE 1:\n",
    "- If you apply L2 weight decay, the network will try to be less sensitive to small changes.\n",
    "- If you apply L1 weight decay, it goes further, the network will try to ignore some inputs altogether. \n",
    "\n",
    "#### NOTE 2:\n",
    "\n",
    "- `kernel_regularizer` acts on the weights, while `bias_initializer` acts on the bias and `activity_regularizer` acts on the y(layer output).\n",
    "\n",
    "- We apply `kernel_regularizer` to punish the weights which are very large causing the network to overfit, after applying `kernel_regularizer` the weights will become smaller.\n",
    "\n",
    "- While we `bias_regularizer` to add a bias so that our bias approaches towards zero.\n",
    "\n",
    "- `activity_regularizer` tries to make the output smaller so as to remove overfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Explanation about L1, L2 and L1_L2 Regularizers:\n",
    "\n",
    "\n",
    "<p>\n",
    "  <img  src=assets/1.png/ >\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "  <img  src=assets/2.png/>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "  <img  src=assets/3.png/>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "  <img  src=assets/4.png/>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "  <img  src=assets/5.png/>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "  <img  src=assets/l1_component.png/>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "  <img  src=assets/5_1.png/>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "  <img  src=assets/l1_deriv.png/>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "  <img  src=assets/5_2.png/>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "  <img  src=assets/6.png/>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "  <img  src=assets/7.png/>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "  <img  src=assets/l2_comp.png/>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "  <img  src=assets/8.png/>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "  <img  src=assets/l1_deriv.png/>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "  <img  src=assets/9.png/>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "  <img  src=assets/l2_deriv.png/>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "  <img  src=assets/10.png/>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "  <img  src=assets/11.png/>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "  <img  src=assets/12.png/>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "  <img  src=assets/elastic_net.png/>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "  <img  src=assets/13.png/>\n",
    "</p>\n",
    "\n",
    "[REFERENCES: Many thanks to Christian for his comprehensive article](https://www.machinecurve.com/index.php/2020/01/21/what-are-l1-l2-and-elastic-net-regularization-in-neural-networks/)\n",
    "\n",
    "[How to use L1, L2 and Elastic Net Regularization with Keras?](https://www.machinecurve.com/index.php/2020/01/23/how-to-use-l1-l2-and-elastic-net-regularization-with-keras/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
