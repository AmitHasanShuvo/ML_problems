{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classification with sigmoid activation function\n",
    "\n",
    "Suppose we are training a model for a binary classification problem with a sigmoid activation function.\n",
    "\n",
    "Given a training example with input $x^{(i)}$, the model will output a float between 0 and 1. Based on whether this float is less than or greater than our \"threshold\" (which by default is set at 0.5), we round the float to get the predicted classification $y_{pred}$ from the model.\n",
    "\n",
    "The accuracy metric compares the value of $y_{pred}$ on each training example with the true output, the one-hot coded vector $y_{true}^{(i)}$ from our training data.\n",
    "\n",
    "Let $$\\delta(y_{pred}^{(i)},y_{true}^{(i)}) = \\begin{cases} 1 & y_{pred}=y_{true}\\\\\n",
    "0 & y_{pred}\\neq y_{true} \\end{cases}$$\n",
    "\n",
    "The accuracy metric  computes the mean of $\\delta(y_{pred}^{(i)},y_{true}^{(i)})$ over all training examples.\n",
    "\n",
    "$$ accuracy = \\frac{1}{N} \\sum_{i=1}^N \\delta(y_{pred}^{(i)},y_{true}^{(i)}) $$\n",
    "\n",
    "This is implemented in the backend of Keras as follows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.6666667>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = tf.constant([0.0, 1.0, 1.0])\n",
    "y_pred = tf.constant([0.4, 0.7, 0.3])\n",
    "\n",
    "accuracy = K.mean(K.equal(y_true, K.round(y_pred)))\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Classification\n",
    "\n",
    "Now suppose we are training a model for a classification problem which should sort data into $m>2$ different classes using a softmax activation function in the last layer.\n",
    "\n",
    "Given a training example with input $x^{(i)}$, the model will output a tensor of probabilities $p_1, p_2, \\dots p_m$, giving the likelihood (according to the model) that $x^{(i)}$ falls into each class.\n",
    "\n",
    "The accuracy metric works by determining the largest argument in the $y_{pred}^{(i)}$ tensor, and compares its index to the index of the maximum value of $y_{true}^{(i)}$ to determine $\\delta(y_{pred}^{(i)},y_{true}^{(i)})$. It then computes the accuracy in the same way as for the binary classification case.\n",
    "\n",
    "$$ accuracy = \\frac{1}{N} \\sum_{i=1}^N \\delta(y_{pred}^{(i)},y_{true}^{(i)}) $$\n",
    "\n",
    "In the backend of Keras, the accuracy metric is implemented slightly differently depending on whether we have a binary classification problem ($m=2$) or a categorical classifcation problem. Note that the accuracy for binary classification problems is the same, no matter if we use a sigmoid or softmax activation function to obtain the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.6666667>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Binary classification with softmax\n",
    "\n",
    "y_true = tf.constant([[0.0,1.0],[1.0,0.0],[1.0,0.0],[0.0,1.0]])\n",
    "y_pred = tf.constant([[0.4,0.6], [0.3,0.7], [0.05,0.95],[0.33,0.67]])\n",
    "accuracy = K.mean(K.equal(y_true, K.round(y_pred)))\n",
    "accuracy\n",
    "\n",
    "# Categorical classification with m>2\n",
    "\n",
    "y_true = tf.constant([[0.0,1.0,0.0,0.0],[1.0,0.0,0.0,0.0],[0.0,0.0,1.0,0.0]])\n",
    "y_pred = tf.constant([[0.4,0.6,0.0,0.0], [0.3,0.2,0.1,0.4], [0.05,0.35,0.5,0.1]])\n",
    "accuracy = K.mean(K.equal(K.argmax(y_true, axis=-1), K.argmax(y_pred, axis=-1)))\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Activation Functions with Numpy:\n",
    "\n",
    "#### Sigmoid Activation Function:\n",
    "\n",
    "Using a mathematical definition, the sigmoid function takes any range real number and returns the output value which falls in the range of 0 to 1. Based on the convention, the output value is expected to be in the range of -1 to 1. The sigmoid function produces an “S” shaped curve. Mathematically, sigmoid is represented as:\n",
    "\n",
    "$$ f (x) =  \\frac{\\mathrm{1} }{\\mathrm{1} + e^- x }  $$ \n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img  src=assets/sigmoid.png/>\n",
    "</p>\n",
    "\n",
    "##### Properties of the Sigmoid Function:\n",
    "\n",
    " - The sigmoid function takes in real numbers in any range and returns a real-valued output.\n",
    " - The first derivative of the sigmoid function will be non-negative (greater than or equal to zero) or non-positive (less than or equal to Zero).\n",
    " - It appears in the output layers of the Deep Learning architectures, and is used for predicting probability based outputs and has been successfully implemented in binary classification problems, logistic regression tasks as well as other neural network applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid():\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return (1/ 1+np.exp(-x))\n",
    "    \n",
    "    def gradient(self, x):\n",
    "        return self.__call__(x) * (1 - self.__call__(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.20274658e+04, 1.80347449e+04, 1.47657816e+04, 1.20893807e+04,\n",
       "       9.89812906e+03, 8.10408393e+03, 6.63524401e+03, 5.43265959e+03,\n",
       "       4.44806675e+03, 3.64195031e+03, 2.98195799e+03, 2.44160198e+03,\n",
       "       1.99919590e+03, 1.63698443e+03, 1.34043076e+03, 1.09763316e+03,\n",
       "       8.98847292e+02, 7.36095189e+02, 6.02845038e+02, 4.93749041e+02,\n",
       "       4.04428793e+02, 3.31299560e+02, 2.71426407e+02, 2.22406416e+02,\n",
       "       1.82272242e+02, 1.49413159e+02, 1.22510418e+02, 1.00484316e+02,\n",
       "       8.24508687e+01, 6.76863310e+01, 5.55981500e+01, 4.57011845e+01,\n",
       "       3.75982344e+01, 3.09641000e+01, 2.55325302e+01, 2.10855369e+01,\n",
       "       1.74446468e+01, 1.44637380e+01, 1.20231764e+01, 1.00250135e+01,\n",
       "       8.38905610e+00, 7.04964746e+00, 5.95303242e+00, 5.05519997e+00,\n",
       "       4.32011692e+00, 3.71828183e+00, 3.22554093e+00, 2.82211880e+00,\n",
       "       2.49182470e+00, 2.22140276e+00, 2.00000000e+00, 1.81873075e+00,\n",
       "       1.67032005e+00, 1.54881164e+00, 1.44932896e+00, 1.36787944e+00,\n",
       "       1.30119421e+00, 1.24659696e+00, 1.20189652e+00, 1.16529889e+00,\n",
       "       1.13533528e+00, 1.11080316e+00, 1.09071795e+00, 1.07427358e+00,\n",
       "       1.06081006e+00, 1.04978707e+00, 1.04076220e+00, 1.03337327e+00,\n",
       "       1.02732372e+00, 1.02237077e+00, 1.01831564e+00, 1.01499558e+00,\n",
       "       1.01227734e+00, 1.01005184e+00, 1.00822975e+00, 1.00673795e+00,\n",
       "       1.00551656e+00, 1.00451658e+00, 1.00369786e+00, 1.00302755e+00,\n",
       "       1.00247875e+00, 1.00202943e+00, 1.00166156e+00, 1.00136037e+00,\n",
       "       1.00111378e+00, 1.00091188e+00, 1.00074659e+00, 1.00061125e+00,\n",
       "       1.00050045e+00, 1.00040973e+00, 1.00033546e+00, 1.00027465e+00,\n",
       "       1.00022487e+00, 1.00018411e+00, 1.00015073e+00, 1.00012341e+00,\n",
       "       1.00010104e+00, 1.00008272e+00, 1.00006773e+00, 1.00005545e+00])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.arange(-10, 10, 0.2)\n",
    "\n",
    "y = Sigmoid()\n",
    "y(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax Activation Function:\n",
    "\n",
    "The Softmax function is used for prediction in multi-class models where it returns probabilities of each class in a group of different classes, with the target class having the highest probability. The calculated probabilities are then helpful in determining the target class for the given inputs. Mathematically, softmax is represented as:\n",
    "\n",
    "$$ softmax(x)_i = \\frac{exp(x_i)}{\\sum_{j}^{ }exp(x_j))} $$\n",
    "\n",
    "##### What does the Softmax function look like?\n",
    "\n",
    "Assume that you have values from $x_1, x_2, \\ldots, x_k$. The Softmax function for these values would be:\n",
    "\n",
    "$\\ln{\\sum_{i=1}^k e^{x_i}}$\n",
    "\n",
    "##### What is the Softmax function doing?\n",
    "\n",
    "*It is approximating the max function*. Can you see why? Let us call the largest $x_i$ value $x_{max}.$ Now, we are taking exponential so $e^{x_{max}}$ will be much larger than any $e^{x_i}$.\n",
    "\n",
    "$\\ln{\\sum_{i=0}^k e^{x_i}} \\approx \\ln e^{x_{max}}$\n",
    "$\\ln{\\sum_{i=0}^k e^{x_i}} \\approx x_{max}$\n",
    "\n",
    "Look at the below graph for a comparison between max(0, x)(red) and softmax(0, x)(blue).\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img  src=assets/softmax.png/  height=\"600px\" width=\"600px\">\n",
    "</p>\n",
    "\n",
    "##### Why is it called Softmax?\n",
    "\n",
    "* It is an approximation of Max.\n",
    "* It is a *soft/smooth* approximation of max. Notice how it approximates the sharp corner at 0 using a smooth curve.\n",
    "\n",
    "##### What is the purpose of Softmax?\n",
    "\n",
    "Softmax gives us the [differentiable](https://en.wikipedia.org/wiki/Differentiable_function) approximation of a [non-differentiable](https://math.stackexchange.com/questions/1329252/is-max0-x-a-differentiable-function) function max. Why is that important? **For optimizing models, including machine learning models, it is required that functions describing the model be differentiable. So if we want to optimize a model which uses the max function then we can do that by replacing the max with softmax.**\n",
    "\n",
    "\n",
    "##### But, What about the Softmax Activation Function name?\n",
    "\n",
    "Here are my guesses on why the Softmax Activation function has the word “Softmax” in it:\n",
    "\n",
    "* Softmax Activation function looks very similar to the Softmax function. Notice the denominator. $f(x_i)=\\frac{e^{x_i}}{\\sum_{i=0}^k e^{x_i}}$\n",
    "\n",
    "* Softmax Activation function highlights the largest input and suppresses all the significantly [smaller ones](https://en.wikipedia.org/wiki/Softmax_function#Example) in certain conditions. In this way, it behaves similar to the softmax function.\n",
    "\n",
    "##### Properties of the Softmax Function:\n",
    "\n",
    "- The Softmax function produces an output which is a range of values between 0 and 1, with the sum of the probabilities been equal to 1.\n",
    "- The main difference between the Sigmoid and Softmax functions is that Sigmoid is used in binary classification while the Softmax is used for multi-class tasks.\n",
    "\n",
    "\n",
    "[REFERENCES](https://medium.com/data-science-bootcamp/softmax-function-beyond-the-basics-51f09ce11154)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef softmax(x):\\n    # Compute softmax values for each sets of scores in x.\\n    e_x = np.exp(x - np.max(x))\\n    return e_x / e_x.sum()\\n    \\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Softmax():\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return np.exp(x)/np.sum(np.exp(x), axis = 1, keepdims=True)\n",
    "    \n",
    "    def gradient(self, x):\n",
    "        p = self.__call__(x)\n",
    "        return p * (1-p)\n",
    "\n",
    "# Taking care of numerical stability\n",
    "\n",
    "\"\"\"\n",
    "def softmax(x):\n",
    "    # Compute softmax values for each sets of scores in x.\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00626879, 0.01704033, 0.04632042, 0.93037047],\n",
       "       [0.01203764, 0.08894682, 0.24178252, 0.65723302],\n",
       "       [0.00626879, 0.01704033, 0.04632042, 0.93037047]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([0.3, 0.7, 0.9])\n",
    "x2 = np.array([[1, 2, 3, 6],  # sample 1\n",
    "               [2, 4, 5, 6],  # sample 2\n",
    "               [1, 2, 3, 6]]) # sample 1 again(!)\n",
    "y = Softmax()\n",
    "y(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReLu Activation Function:\n",
    "\n",
    "The Rectified linear unit (ReLu) activation function has been the most widely used activation function for deep learning applications with state-of-the-art results. It usually achieves better performance and generalization in deep learning compared to the sigmoid activation function.\n",
    "\n",
    "$$ f (x) =  max(x, 0) $$ \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img  src=assets/relu.png/  height=\"600px\" width=\"600px\">\n",
    "</p>\n",
    "\n",
    "**Nature** :- Non-linear, which means we can easily backpropagate the errors and have multiple layers of neurons being activated by the ReLU function.\n",
    "\n",
    "**Uses** :- ReLu is less computationally expensive than tanh and sigmoid because it involves simpler mathematical operations. At a time only a few neurons are activated making the network sparse making it efficient and easy for computation.\n",
    "\n",
    "**It avoids and rectifies vanishing gradient problem.** Almost all deep learning Models use ReLu nowadays.\n",
    "But its limitation is that it should only be used within Hidden layers of a Neural Network Model.\n",
    "\n",
    "Another problem with ReLu is that **ReLU units can be fragile during training and can \"die\".** For example, a large gradient flowing through a ReLU neuron could cause the weights to update in such a way that the neuron will never activate on any datapoint again. If this happens, then the gradient flowing through the unit will forever be zero from that point on. That is, the ReLU units can irreversibly die during training since they can get knocked off the data manifold. For example, you may find that as much as 40% of your network can be \"dead\" (i.e. neurons that never activate across the entire training dataset) if the learning rate is set too high. With a proper setting of the learning rate this is less frequently an issue.\n",
    "\n",
    "To fix this problem another modification was introduced called **Leaky ReLu** to fix the problem of dying neurons. It introduces a small slope to keep the updates alive.\n",
    "\n",
    "The leak helps to increase the range of the ReLU function. Usually, the value of alpha is 0.01 or so.\n",
    "\n",
    "**When a is not 0.01 then it is called Randomized ReLU.**\n",
    "\n",
    "Therefore the range of the Leaky ReLU is (-infinity to infinity).\n",
    "\n",
    "Both Leaky and Randomized ReLU functions are monotonic(A function which is either entirely non-increasing or non-decreasing.) in nature. Also, their derivatives also monotonic in nature.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img  src=assets/lrelu.png/  height=\"600px\" width=\"600px\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLu:\n",
    "    def __call__(self, x):\n",
    "        return np.where(x>=0, x, 0)\n",
    "    \n",
    "    def gradient(self, x):\n",
    "        return np.where(x>=0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.99694447, 0.        ],\n",
       "       [0.        , 0.        , 0.81905501],\n",
       "       [0.        , 0.18253543, 0.92855557]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = np.random.uniform(-1, 1, (3,3))\n",
    "\n",
    "relu = ReLu()\n",
    "relu(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Leaky_ReLu:\n",
    "    def __init__(self, alpha = 0.01):\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return np.where(x>=0, x, self.alpha*x)\n",
    "    \n",
    "    def gradient(self, x):\n",
    "        return np.where(x>=0, 1, self.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.89244738e-01, -2.48947306e-03,  6.64513815e-01],\n",
       "       [-1.40195921e-04,  3.37395609e-01, -9.66792375e-03],\n",
       "       [ 1.58964744e-01, -3.73088069e-03, -5.08719887e-03]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = np.random.uniform(-1, 1, (3,3))\n",
    "\n",
    "lrelu = Leaky_ReLu()\n",
    "lrelu(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tanh Activation Function:\n",
    "\n",
    "tanh is also like logistic sigmoid but better. The range of the tanh function is from (-1 to 1). tanh is also sigmoidal (s - shaped).\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img  src=assets/tanh_formula.jpg/>\n",
    "</p>\n",
    "\n",
    "### Tanh also suffers from Vanishing Gradient Problem\n",
    "<p align=\"center\">\n",
    "  <img  src=assets/tanh.jpg/>\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img  src=assets/tanh_and_gradient.jpg/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TanH():\n",
    "    def __call__(self, x):\n",
    "        return 2 / (1 + np.exp(-2*x)) - 1\n",
    "\n",
    "    def gradient(self, x):\n",
    "        return 1 - np.power(self.__call__(x), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.48660719, -0.70912964,  0.18195864],\n",
       "       [-0.36787464, -0.7005745 ,  0.70007243],\n",
       "       [ 0.1619226 ,  0.69155896, -0.16911645]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = np.random.uniform(-1, 1, (3,3))\n",
    "\n",
    "tanh = TanH()\n",
    "tanh(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Metrics\n",
    "\n",
    "#### Crossentropy Loss:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img  src=assets/CE_loss.png/ height=\"600px\" width=\"600px\">\n",
    "</p>\n",
    "\n",
    "\n",
    "**NOTE** :\n",
    "- Also see [Kullback-Leibler Divergence](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained) [Very often in Probability and Statistics we'll replace observed data or a complex distributions with a simpler, approximating distribution. KL Divergence helps us to measure just how much information we lose when we choose an approximation.] (which explains how the formula for CE originated) \n",
    "- [Cross-Entropy explained qualitatively](https://stats.stackexchange.com/questions/80967/qualitively-what-is-cross-entropy)\n",
    "- [Shannon's entropy](https://stats.stackexchange.com/questions/87182/what-is-the-role-of-the-logarithm-in-shannons-entropy/87194#87194)\n",
    "- [Bayesian Surprise and Maximum Entropy Distribution](https://stats.stackexchange.com/questions/66186/statistical-interpretation-of-maximum-entropy-distribution/245198#245198)\n",
    "- [Intuition on the Kullback-Leibler (KL) Divergence](https://stats.stackexchange.com/questions/188903/intuition-on-the-kullback-leibler-kl-divergence/189758#189758)\n",
    "\n",
    "##### Multi-Class Classification:\n",
    "\n",
    "One-of-many classification. Each sample can belong to ONE of $ C $ classes. The CNN will have $C$ output neurons that can be gathered in a vector $s$ (Scores). The target (ground truth) vector $t$ will be a one-hot vector with a positive class and $C - 1$ negative classes.   \n",
    "This task is treated as a single classification problem of samples in one of $C$ classes.\n",
    "\n",
    "##### Multi-Label Classification:\n",
    "\n",
    "Each sample can belong to more than one class. The CNN will have as well $C$ output neurons. The target vector $t$ can have more than a positive class, so it will be a vector of 0s and 1s with $C$ dimensionality.   \n",
    "This task is treated as $C$ different binary (C’ = 2, t’ = 0) or  (t’ = 1) and independent classification problems, where each output neuron decides if a sample belongs to a class or not.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img  src=assets/multiclass_multilabel.png height=\"600px\" width=\"600px\">\n",
    "</p>\n",
    "\n",
    "The Cross-Entropy Loss can be defined as:\n",
    "\n",
    "$$ CE = -\\sum_{i}^{C}t_{i} log (s_{i}) $$\n",
    "\n",
    "Where $t_i$ and $s_i$ are the groundtruth and the CNN score for each class _i_ in $C$. As **usually an activation function (Sigmoid / Softmax) is applied to the scores before the CE Loss computation**, we write $f(s_i)$ to refer to the activations.   \n",
    "\n",
    "In a **binary classification problem**, where $C’ = 2$, the Cross Entropy Loss can be defined also as:\n",
    "\n",
    "$$CE = -\\sum_{i=1}^{C'=2}t_{i} log (s_{i}) = -t_{1} log(s_{1}) - (1 - t_{1}) log(1 - s_{1})$$\n",
    "\n",
    "where it’s assumed that there are two classes: $C_1$ and $C_2$. $t_1$ [0,1] and $s_1$ are the groundtruth and the score for $C_1$, and $t_2 =  1 - t_1$ and $s_2 =  1 - s_1$ are the groundtruth and the score for $C_2$. That is the case when we split a Multi-Label classification problem in $C$ binary classification problems. See next Binary Cross-Entropy Loss section for more details.\n",
    "\n",
    "\n",
    "**Logistic Loss** and **Multinomial Logistic Loss** are other names for **Cross-Entropy loss**.\n",
    "\n",
    "### Categorical Cross-Entropy Loss:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img  src=assets/softmax_CE_pipeline.png height=\"600px\" width=\"600px\">\n",
    "</p>\n",
    "\n",
    "Also called **Softmax Loss**. It is a **Softmax activation** plus a **Cross-Entropy loss**. If we use this loss, we will train a CNN to output a probability over the $C$ classes for each image. It is used for multi-class classification.\n",
    "\n",
    "In the specific (and usual) case of Multi-Class classification the labels are one-hot, so only the positive class $C_p$ keeps its term in the loss. There is only one element of the Target vector $t$ which is not zero $t_i = t_p$. So discarding the elements of the summation which are zero due to target labels, we can write:\n",
    "\n",
    "$$ CE = -log\\left ( \\frac{e^{s_{p}}}{\\sum_{j}^{C} e^{s_{j}}}\\right ) $$\n",
    "\n",
    "where **Sp** is the CNN score for the positive class.\n",
    "\n",
    "### Binary Cross-Entropy Loss:\n",
    "\n",
    "Also called **Sigmoid Cross-Entropy loss**. It is a **Sigmoid activation** plus a **Cross-Entropy loss**. Unlike **Softmax loss** it is independent for each vector component (class), meaning that the loss computed for every CNN output vector component is not affected by other component values. That’s why it is used for **multi-label classification**, were the insight of an element belonging to a certain class should not influence the decision for another class.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img  src=assets/sigmoid_CE_pipeline.png height=\"600px\" width=\"600px\">\n",
    "</p>\n",
    "\n",
    "\n",
    "It’s called **Binary Cross-Entropy Loss** because it sets up a binary classification problem between C’ = 2 classes for every class in $C$, as explained above. So when using this Loss, the formulation of **Cross Entroypy Loss** for binary problems is often used:\n",
    "\n",
    "$$CE = -\\sum_{i=1}^{C'=2}t_{i} log (f(s_{i})) = -t_{1} log(f(s_{1})) - (1 - t_{1}) log(1 - f(s_{1}))$$\n",
    "\n",
    "This would be the pipeline for each one of the $C$ clases. We set $C$ independent binary classification problems (C’ = 2). Then we sum up the loss over the different binary problems: We sum up the gradients of every binary problem to backpropagate, and the losses to monitor the global loss. $s_1$ and $t_1$ are the score and the gorundtruth label for the class $C_1$, which is also the class $C_i$ in $C$. $s_2 = 1 - s_1$ and $t_2 = 1 - t_1$ are the score and the groundtruth label of the class $C_2$, which is not a “class” in our original problem with $C$ classes, but a class we create to set up the binary problem with $C_1 = C_i$. We can understand it as a background class.\n",
    "\n",
    "The loss can be expressed as:\n",
    "\n",
    "$$CE = \\left\\{\\begin{matrix} & - log(f(s_{1})) & & if & t_{1} = 1 \\\\ & - log(1 - f(s_{1})) & & if & t_{1} = 0 \\end{matrix}\\right.$$\n",
    "\n",
    "where $t_1 = 1$ means that the class $C_1 = C_i$ is positive for this sample.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
