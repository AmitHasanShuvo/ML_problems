{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classification with sigmoid activation function\n",
    "\n",
    "Suppose we are training a model for a binary classification problem with a sigmoid activation function.\n",
    "\n",
    "Given a training example with input $x^{(i)}$, the model will output a float between 0 and 1. Based on whether this float is less than or greater than our \"threshold\" (which by default is set at 0.5), we round the float to get the predicted classification $y_{pred}$ from the model.\n",
    "\n",
    "The accuracy metric compares the value of $y_{pred}$ on each training example with the true output, the one-hot coded vector $y_{true}^{(i)}$ from our training data.\n",
    "\n",
    "Let $$\\delta(y_{pred}^{(i)},y_{true}^{(i)}) = \\begin{cases} 1 & y_{pred}=y_{true}\\\\\n",
    "0 & y_{pred}\\neq y_{true} \\end{cases}$$\n",
    "\n",
    "The accuracy metric  computes the mean of $\\delta(y_{pred}^{(i)},y_{true}^{(i)})$ over all training examples.\n",
    "\n",
    "$$ accuracy = \\frac{1}{N} \\sum_{i=1}^N \\delta(y_{pred}^{(i)},y_{true}^{(i)}) $$\n",
    "\n",
    "This is implemented in the backend of Keras as follows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.6666667>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = tf.constant([0.0, 1.0, 1.0])\n",
    "y_pred = tf.constant([0.4, 0.7, 0.3])\n",
    "\n",
    "accuracy = K.mean(K.equal(y_true, K.round(y_pred)))\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Classification\n",
    "\n",
    "Now suppose we are training a model for a classification problem which should sort data into $m>2$ different classes using a softmax activation function in the last layer.\n",
    "\n",
    "Given a training example with input $x^{(i)}$, the model will output a tensor of probabilities $p_1, p_2, \\dots p_m$, giving the likelihood (according to the model) that $x^{(i)}$ falls into each class.\n",
    "\n",
    "The accuracy metric works by determining the largest argument in the $y_{pred}^{(i)}$ tensor, and compares its index to the index of the maximum value of $y_{true}^{(i)}$ to determine $\\delta(y_{pred}^{(i)},y_{true}^{(i)})$. It then computes the accuracy in the same way as for the binary classification case.\n",
    "\n",
    "$$ accuracy = \\frac{1}{N} \\sum_{i=1}^N \\delta(y_{pred}^{(i)},y_{true}^{(i)}) $$\n",
    "\n",
    "In the backend of Keras, the accuracy metric is implemented slightly differently depending on whether we have a binary classification problem ($m=2$) or a categorical classifcation problem. Note that the accuracy for binary classification problems is the same, no matter if we use a sigmoid or softmax activation function to obtain the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.6666667>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Binary classification with softmax\n",
    "\n",
    "y_true = tf.constant([[0.0,1.0],[1.0,0.0],[1.0,0.0],[0.0,1.0]])\n",
    "y_pred = tf.constant([[0.4,0.6], [0.3,0.7], [0.05,0.95],[0.33,0.67]])\n",
    "accuracy = K.mean(K.equal(y_true, K.round(y_pred)))\n",
    "accuracy\n",
    "\n",
    "# Categorical classification with m>2\n",
    "\n",
    "y_true = tf.constant([[0.0,1.0,0.0,0.0],[1.0,0.0,0.0,0.0],[0.0,0.0,1.0,0.0]])\n",
    "y_pred = tf.constant([[0.4,0.6,0.0,0.0], [0.3,0.2,0.1,0.4], [0.05,0.35,0.5,0.1]])\n",
    "accuracy = K.mean(K.equal(K.argmax(y_true, axis=-1), K.argmax(y_pred, axis=-1)))\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Activation Functions with Numpy:\n",
    "\n",
    "#### Sigmoid Activation Function:\n",
    "\n",
    "Using a mathematical definition, the sigmoid function takes any range real number and returns the output value which falls in the range of 0 to 1. Based on the convention, the output value is expected to be in the range of -1 to 1. The sigmoid function produces an “S” shaped curve. Mathematically, sigmoid is represented as:\n",
    "\n",
    "$$ f (x) =  \\frac{\\mathrm{1} }{\\mathrm{1} + e^- x }  $$ \n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img  src=assets/sigmoid.png/>\n",
    "</p>\n",
    "\n",
    "##### Properties of the Sigmoid Function:\n",
    "\n",
    " - The sigmoid function takes in real numbers in any range and returns a real-valued output.\n",
    " - The first derivative of the sigmoid function will be non-negative (greater than or equal to zero) or non-positive (less than or equal to Zero).\n",
    " - It appears in the output layers of the Deep Learning architectures, and is used for predicting probability based outputs and has been successfully implemented in binary classification problems, logistic regression tasks as well as other neural network applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid():\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return (1/ 1+np.exp(-x))\n",
    "    \n",
    "    def gradient(self, x):\n",
    "        return self.__call__(x) * (1 - self.__call__(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.20274658e+04, 1.80347449e+04, 1.47657816e+04, 1.20893807e+04,\n",
       "       9.89812906e+03, 8.10408393e+03, 6.63524401e+03, 5.43265959e+03,\n",
       "       4.44806675e+03, 3.64195031e+03, 2.98195799e+03, 2.44160198e+03,\n",
       "       1.99919590e+03, 1.63698443e+03, 1.34043076e+03, 1.09763316e+03,\n",
       "       8.98847292e+02, 7.36095189e+02, 6.02845038e+02, 4.93749041e+02,\n",
       "       4.04428793e+02, 3.31299560e+02, 2.71426407e+02, 2.22406416e+02,\n",
       "       1.82272242e+02, 1.49413159e+02, 1.22510418e+02, 1.00484316e+02,\n",
       "       8.24508687e+01, 6.76863310e+01, 5.55981500e+01, 4.57011845e+01,\n",
       "       3.75982344e+01, 3.09641000e+01, 2.55325302e+01, 2.10855369e+01,\n",
       "       1.74446468e+01, 1.44637380e+01, 1.20231764e+01, 1.00250135e+01,\n",
       "       8.38905610e+00, 7.04964746e+00, 5.95303242e+00, 5.05519997e+00,\n",
       "       4.32011692e+00, 3.71828183e+00, 3.22554093e+00, 2.82211880e+00,\n",
       "       2.49182470e+00, 2.22140276e+00, 2.00000000e+00, 1.81873075e+00,\n",
       "       1.67032005e+00, 1.54881164e+00, 1.44932896e+00, 1.36787944e+00,\n",
       "       1.30119421e+00, 1.24659696e+00, 1.20189652e+00, 1.16529889e+00,\n",
       "       1.13533528e+00, 1.11080316e+00, 1.09071795e+00, 1.07427358e+00,\n",
       "       1.06081006e+00, 1.04978707e+00, 1.04076220e+00, 1.03337327e+00,\n",
       "       1.02732372e+00, 1.02237077e+00, 1.01831564e+00, 1.01499558e+00,\n",
       "       1.01227734e+00, 1.01005184e+00, 1.00822975e+00, 1.00673795e+00,\n",
       "       1.00551656e+00, 1.00451658e+00, 1.00369786e+00, 1.00302755e+00,\n",
       "       1.00247875e+00, 1.00202943e+00, 1.00166156e+00, 1.00136037e+00,\n",
       "       1.00111378e+00, 1.00091188e+00, 1.00074659e+00, 1.00061125e+00,\n",
       "       1.00050045e+00, 1.00040973e+00, 1.00033546e+00, 1.00027465e+00,\n",
       "       1.00022487e+00, 1.00018411e+00, 1.00015073e+00, 1.00012341e+00,\n",
       "       1.00010104e+00, 1.00008272e+00, 1.00006773e+00, 1.00005545e+00])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.arange(-10, 10, 0.2)\n",
    "\n",
    "y = Sigmoid()\n",
    "y(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax Activation Function:\n",
    "\n",
    "The Softmax function is used for prediction in multi-class models where it returns probabilities of each class in a group of different classes, with the target class having the highest probability. The calculated probabilities are then helpful in determining the target class for the given inputs. Mathematically, softmax is represented as:\n",
    "\n",
    "$$ softmax(x)_i = \\frac{exp(x_i)}{\\sum_{j}^{ }exp(x_j))} $$\n",
    "\n",
    "##### What does the Softmax function look like?\n",
    "\n",
    "Assume that you have values from $x_1, x_2, \\ldots, x_k$. The Softmax function for these values would be:\n",
    "\n",
    "$\\ln{\\sum_{i=1}^k e^{x_i}}$\n",
    "\n",
    "##### What is the Softmax function doing?\n",
    "\n",
    "*It is approximating the max function*. Can you see why? Let us call the largest $x_i$ value $x_{max}.$ Now, we are taking exponential so $e^{x_{max}}$ will be much larger than any $e^{x_i}$.\n",
    "\n",
    "$\\ln{\\sum_{i=0}^k e^{x_i}} \\approx \\ln e^{x_{max}}$\n",
    "$\\ln{\\sum_{i=0}^k e^{x_i}} \\approx x_{max}$\n",
    "\n",
    "Look at the below graph for a comparison between max(0, x)(red) and softmax(0, x)(blue).\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img  src=assets/softmax.png/>\n",
    "</p>\n",
    "\n",
    "##### Why is it called Softmax?\n",
    "\n",
    "* It is an approximation of Max.\n",
    "* It is a *soft/smooth* approximation of max. Notice how it approximates the sharp corner at 0 using a smooth curve.\n",
    "\n",
    "##### What is the purpose of Softmax?\n",
    "\n",
    "Softmax gives us the [differentiable](https://en.wikipedia.org/wiki/Differentiable_function) approximation of a [non-differentiable](https://math.stackexchange.com/questions/1329252/is-max0-x-a-differentiable-function) function max. Why is that important? **For optimizing models, including machine learning models, it is required that functions describing the model be differentiable. So if we want to optimize a model which uses the max function then we can do that by replacing the max with softmax.**\n",
    "\n",
    "\n",
    "##### But, What about the Softmax Activation Function name?\n",
    "\n",
    "Here are my guesses on why the Softmax Activation function has the word “Softmax” in it:\n",
    "\n",
    "* Softmax Activation function looks very similar to the Softmax function. Notice the denominator. $f(x_i)=\\frac{e^{x_i}}{\\sum_{i=0}^k e^{x_i}}$\n",
    "\n",
    "* Softmax Activation function highlights the largest input and suppresses all the significantly [smaller ones](https://en.wikipedia.org/wiki/Softmax_function#Example) in certain conditions. In this way, it behaves similar to the softmax function.\n",
    "\n",
    "\n",
    "\n",
    "[REFERENCES](https://medium.com/data-science-bootcamp/softmax-function-beyond-the-basics-51f09ce11154)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax():\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return np.exp(x)/np.sum(np.exp(x), axis = 0)\n",
    "    \n",
    "    def gradient(self, x):\n",
    "        p = self.__call__(x)\n",
    "        return p * (1-p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[0.3, 0.7, 0.9]])\n",
    "\n",
    "y = Softmax()\n",
    "y(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
