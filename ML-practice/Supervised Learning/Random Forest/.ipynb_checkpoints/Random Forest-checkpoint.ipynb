{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding a Decision Tree\n",
    "\n",
    "A decision tree is the building block of a random forest and is an intuitive model. \n",
    "\n",
    "We can think of a decision tree as a series of yes/no questions asked about our data eventually leading to a predicted class (or continuous value in the case of regression). This is an interpretable model because it makes classifications much like we do: we ask a sequence of queries about the available data we have until we arrive at a decision (in an ideal world).\n",
    "\n",
    "The technical details of a decision tree are in how the questions about the data are formed. \n",
    "\n",
    "**In the CART algorithm, a decision tree is built by determining the questions (called splits of nodes) that, when answered, lead to the greatest reduction in Gini Impurity. What this means is the decision tree tries to form nodes containing a high proportion of samples (data points) from a single class by finding values in the features that cleanly divide the data into classes.**\n",
    "\n",
    "<p>\n",
    "    <img src = assets/1.png height = 400px width = 400px>\n",
    "</p>\n",
    "\n",
    "Our data only has two features (predictor variables), x1 and x2 with 6 data points — samples — divided into 2 different labels. Although this problem is simple, it’s not linearly separable, which means that we can’t draw a single straight line through the data to classify the points.\n",
    "\n",
    "\n",
    "We can however draw a series of straight lines that divide the data points into boxes, which we’ll call nodes. In fact, this is what a decision tree does during training. Effectively, a decision tree is a non-linear model built by constructing many linear boundaries.\n",
    "\n",
    "#### Visualizing a Decision Tree\n",
    "\n",
    "So what’s actually going on when we train a decision tree?\n",
    "\n",
    "<p>\n",
    "    <img src = assets/2.png height = 600px width = 600px>\n",
    "</p>\n",
    "\n",
    "All the nodes, except the leaf nodes (colored terminal nodes), have 5 parts:\n",
    "\n",
    "- Question asked about the data based on a value of a feature. Each question has either a True or False answer that splits the node. Based on the answer to the question, a data point moves down the tree.\n",
    "- `gini`: The Gini Impurity of the node. The average weighted Gini Impurity decreases as we move down the tree.\n",
    "- `samples`: The number of observations in the node.\n",
    "- `value`: The number of samples in each class. For example, the top node has 2 samples in class 0 and 4 samples in class 1.\n",
    "- `class`: The majority classification for points in the node. In the case of leaf nodes, this is the prediction for all samples in the node.\n",
    "\n",
    "**The leaf nodes do not have a question because these are where the final predictions are made. To classify a new point, simply move down the tree, using the features of the point to answer the questions until you arrive at a leaf node where the `class` is the prediction.**\n",
    "\n",
    "To make see the tree in a different way, we can draw the splits built by the decision tree on the original data.\n",
    "\n",
    "<p>\n",
    "    <img src = assets/3.png height = 400px width = 400px>\n",
    "</p>\n",
    "\n",
    "Each split is a single line that divides data points into nodes based on feature values. For this simple problem and with no limit on the maximum depth, the divisions place each point in a node with only points of the same class. (Again, later we’ll see that this perfect division of the training data might not be what we want because it can lead to overfitting.)\n",
    "\n",
    "### Gini Impurity:\n",
    "\n",
    "**The Gini Impurity of a node is the probability that a randomly chosen sample in a node would be incorrectly labeled if it was labeled by the distribution of samples in the node.** For example, in the top (root) node, there is a 44.4% chance of incorrectly classifying a data point chosen at random based on the sample labels in the node. We arrive at this value using the following equation:\n",
    "\n",
    "<p>\n",
    "    <img src = assets/4.png height = 400px width = 400px>\n",
    "</p>\n",
    "\n",
    "\n",
    "The Gini Impurity of a node n is 1 minus the sum over all the classes J (for a binary classification task this is 2) of the fraction of examples in each class p_i squared. That might be a little confusing in words, so let’s work out the Gini impurity of the root node.\n",
    "\n",
    "<p>\n",
    "    <img src = assets/5.png>\n",
    "</p>\n",
    "\n",
    "**At each node, the decision tree searches through the features for the value to split on that results in the greatest reduction in Gini Impurity.**\n",
    "\n",
    "It then repeats this splitting process in a greedy, recursive procedure until it reaches a maximum depth, or each node contains only samples from one class. The weighted total Gini Impurity at each level of tree must decrease. At the second level of the tree, the total weighted Gini Impurity is 0.333:\n",
    "\n",
    "<p>\n",
    "    <img src = assets/6.png>\n",
    "</p>\n",
    "\n",
    "**(The Gini Impurity of each node is weighted by the fraction of points from the parent node in that node.)**\n",
    "\n",
    "Eventually, the weighted total Gini Impurity of the last layer goes to 0 meaning each node is completely pure and there is no chance that a point randomly selected from that node would be misclassified. While this may seem like a positive, it means that the model may potentially be overfitting because the nodes are constructed only using training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use Gini Impurity and when to use Entropy?\n",
    "\n",
    "**They are pretty much the same when it comes to CART Analytics.**\n",
    "\n",
    "Generally, your performance will not change whether you use Gini impurity or Entropy. \n",
    "\n",
    "Laura Elena Raileanu and Kilian Stoffel compared both in \"[Theoretical comparison between the gini index and information gain criteria](https://www.unine.ch/files/live/sites/imi/files/shared/documents/papers/Gini_index_fulltext.pdf)\". The most important remarks were:\n",
    "\n",
    "- It only matters in 2% of the cases whether you use gini impurity or entropy. \n",
    "- Entropy might be a little slower to compute (because it makes use of the logarithm). \n",
    "\n",
    "I was once told that **both metrics exist because they emerged in different disciplines of science.**\n",
    "\n",
    "Also, \n",
    "- Gini is intended for continuous attributes and Entropy is for attributes that occur in classes\n",
    "- Gini is to minimize misclassification while Entropy is for exploratory analysis\n",
    "\n",
    "### [Intuition behing Gini impurity formula](https://stats.stackexchange.com/a/308886/210548)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REFERENCES:\n",
    "- [Random Forest Implementation and Explanation](https://towardsdatascience.com/an-implementation-and-explanation-of-the-random-forest-in-python-77bf308a9b76)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
