{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a decision tree?\n",
    "\n",
    "Decision trees can be used for regression (continuous real-valued output, e.g. predicting the price of a house) or classification (categorical output, e.g. predicting email spam vs. no spam), but here we will focus on classification.\n",
    "\n",
    "A decision tree classifier is a binary tree where predictions are made by traversing the tree from root to leaf — at each node, we go left if a feature is less than a threshold, right otherwise. Finally, each leaf is associated with a class, which is the output of the predictor.\n",
    "\n",
    "### Their types:\n",
    "\n",
    "There is a wide variety of decision trees, such as:\n",
    "- ID3 (Iterative Dichotomiser 3)\n",
    "- CART (Classification and Regression Tree)\n",
    "- CHAID (Chi-squared Automatic Interaction Detector), etc.\n",
    "\n",
    "#### How does it work?\n",
    "\n",
    "- Starts at the root node\n",
    "- Splits data into groups (based on some criteria)\n",
    "- Set a decision at node\n",
    "- Move the data along the respective branches\n",
    "- Repeat the process until a stopping criterion is met (max levels/depth reached, min samples left to split, nothing left to split, etc)\n",
    "\n",
    "#### How to choose root node:\n",
    "\n",
    "This is slightly different in regression and classification trees.\n",
    "\n",
    "**In regression trees**, we chose a splitting point such that there is the greatest reduction in RSS (Residual Sum of Squares).\n",
    "\n",
    "\n",
    "<p>\n",
    "        <img src = assets/1.png/ height = \"400px\" width = \"400px\">\n",
    "</p>\n",
    "\n",
    "Or we can calculate standard deviation reduction of the feature with respect to the training data. Here YR1 and YR2 are mean responses of region 1 and 2. Once we train the tree, we predict the response for a test data using the mean of the training observations in that group.\n",
    "\n",
    "**In Classification trees:**\n",
    "\n",
    "We use Entropy and Information Gain (in ID3). Gini Index for classification in the CART.\n",
    "\n",
    "**Entropy (Shannon’s Entropy)** quantifies the uncertainty of chaos in the group. Higher entropy means higher the disorder. It is denoted by H(x), where x is a vector with probabilities p1, p2, p3…..\n",
    "\n",
    "<p>\n",
    "        <img src = assets/2.png/ height = \"200px\" width = \"200px\">\n",
    "</p>\n",
    "\n",
    "From the above figure, we can see that the entropy (uncertainty) is highest (1) when the probability is 0.5, i.e. 50–50 chances. And entropy is lowest when the probability is 0 or 1, i.e. there is no uncertainty or high chance of occurrence.\n",
    "\n",
    "\n",
    "So, entropy is maximum if in a class there are an equal number of objects from different attributes (like the group has 50 cats and 50 dogs), and this is minimum if the node is pure (like the group has only 100 cats or only 100 dogs). We ultimately want to have minimum entropy for the tree, i.e. pure or uniform classes at the leaf nodes.\n",
    "\n",
    "##### Entropy calculation:\n",
    "\n",
    "<p>\n",
    "        <img src = assets/3.png/ height = \"400px\" width = \"400px\">\n",
    "</p>\n",
    "\n",
    "- S — Current group for which we are interested in calculating entropy.\n",
    "- Pi — Probability of finding that system in ith state, or this turns to the proportion of a number of elements in that split group to the number of elements in the group before splitting(parent group).\n",
    "\n",
    "In this classification tree, while splitting the tree we select those attributes that achieves the greatest reduction in entropy. Now, this reduction (or change) in entropy is measured by **Information Gain** which is given by:\n",
    "\n",
    "<p>\n",
    "        <img src = assets/4.png/ height = \"400px\" width = \"400px\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:\n",
    "\n",
    "The problem is about predicting whether some kid is going to eat a particular type of food given that kids prior eating habits.\n",
    "\n",
    "<p>\n",
    "        <img src = assets/5.png/ height = \"400px\" width = \"400px\">\n",
    "</p>\n",
    "\n",
    "From the above chart, we can see that the food preferences Taste, Temperature and Texture are exploratory variables and Eat (Yes/No) is target variable.\n",
    "\n",
    "Now, we need to construct a top-down decision tree that splits the dataset and finally form a pure group, so we can predict for a new test variable if the kid eats or not.\n",
    "\n",
    "We are going to use the ID3 algorithm for this.\n",
    "\n",
    "<p>\n",
    "        <img src = assets/6.png/ >\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "        <img src = assets/7.png/>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "        <img src = assets/8.png/ >\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "        <img src = assets/8_1.png/ >\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "        <img src = assets/9.png/ >\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "        <img src = assets/10.png/ >\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "        <img src = assets/10_1.png/ >\n",
    "</p>\n",
    "\n",
    "\n",
    "<p>\n",
    "        <img src = assets/11.png/ >\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "        <img src = assets/12.png/ >\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "        <img src = assets/12_1.png/ >\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "        <img src = assets/13.png/ >\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "        <img src = assets/final.png/>\n",
    "</p>\n",
    "\n",
    "\n",
    "#### Conclusion:\n",
    "\n",
    "Finally, what we can conclude is, if the food is sweet the kid is not caring about its Temperature or Texture, he is eating.\n",
    "\n",
    "If the food is Salty he is eating only if the texture is hard. And if the food is Spicy he eats if it is Hot and Hard or Cold and Soft. Bizarre Kid.\n",
    "\n",
    "#### REFERENCES:\n",
    "\n",
    "- [Math behing DT](https://medium.com/@rakendd/building-decision-trees-and-its-math-711862eea1c0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
