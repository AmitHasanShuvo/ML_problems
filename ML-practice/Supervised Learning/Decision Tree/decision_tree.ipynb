{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a decision tree?\n",
    "\n",
    "Decision trees can be used for regression (continuous real-valued output, e.g. predicting the price of a house) or classification (categorical output, e.g. predicting email spam vs. no spam), but here we will focus on classification.\n",
    "\n",
    "A decision tree classifier is a binary tree where predictions are made by traversing the tree from root to leaf — at each node, we go left if a feature is less than a threshold, right otherwise. Finally, each leaf is associated with a class, which is the output of the predictor.\n",
    "\n",
    "### Their types:\n",
    "\n",
    "There is a wide variety of decision trees, such as:\n",
    "- ID3 (Iterative Dichotomiser 3)\n",
    "- CART (Classification and Regression Tree)\n",
    "- CHAID (Chi-squared Automatic Interaction Detector), etc.\n",
    "\n",
    "#### How does it work?\n",
    "\n",
    "- Starts at the root node\n",
    "- Splits data into groups (based on some criteria)\n",
    "- Set a decision at node\n",
    "- Move the data along the respective branches\n",
    "- Repeat the process until a stopping criterion is met (max levels/depth reached, min samples left to split, nothing left to split, etc)\n",
    "\n",
    "#### How to choose root node:\n",
    "\n",
    "This is slightly different in regression and classification trees.\n",
    "\n",
    "**In regression trees**, we chose a splitting point such that there is the greatest reduction in RSS (Residual Sum of Squares).\n",
    "\n",
    "\n",
    "<p>\n",
    "        <img src = assets/1.png/ height = \"400px\" width = \"400px\">\n",
    "</p>\n",
    "\n",
    "Or we can calculate standard deviation reduction of the feature with respect to the training data. Here YR1 and YR2 are mean responses of region 1 and 2. Once we train the tree, we predict the response for a test data using the mean of the training observations in that group.\n",
    "\n",
    "**In Classification trees:**\n",
    "\n",
    "We use Entropy and Information Gain (in ID3). Gini Index for classification in the CART.\n",
    "\n",
    "**Entropy (Shannon’s Entropy)** quantifies the uncertainty of chaos in the group. Higher entropy means higher the disorder. It is denoted by H(x), where x is a vector with probabilities p1, p2, p3…..\n",
    "\n",
    "<p>\n",
    "        <img src = assets/2.png/ height = \"200px\" width = \"200px\">\n",
    "</p>\n",
    "\n",
    "From the above figure, we can see that the entropy (uncertainty) is highest (1) when the probability is 0.5, i.e. 50–50 chances. And entropy is lowest when the probability is 0 or 1, i.e. there is no uncertainty or high chance of occurrence.\n",
    "\n",
    "\n",
    "So, entropy is maximum if in a class there are an equal number of objects from different attributes (like the group has 50 cats and 50 dogs), and this is minimum if the node is pure (like the group has only 100 cats or only 100 dogs). We ultimately want to have minimum entropy for the tree, i.e. pure or uniform classes at the leaf nodes.\n",
    "\n",
    "##### Entropy calculation:\n",
    "\n",
    "<p>\n",
    "        <img src = assets/3.png/ height = \"400px\" width = \"400px\">\n",
    "</p>\n",
    "\n",
    "- S — Current group for which we are interested in calculating entropy.\n",
    "- Pi — Probability of finding that system in ith state, or this turns to the proportion of a number of elements in that split group to the number of elements in the group before splitting(parent group).\n",
    "\n",
    "In this classification tree, while splitting the tree we select those attributes that achieves the greatest reduction in entropy. Now, this reduction (or change) in entropy is measured by **Information Gain** which is given by:\n",
    "\n",
    "<p>\n",
    "        <img src = assets/4.png/ height = \"400px\" width = \"400px\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:\n",
    "\n",
    "The problem is about predicting whether some kid is going to eat a particular type of food given that kids prior eating habits.\n",
    "\n",
    "<p>\n",
    "        <img src = assets/5.png/ height = \"400px\" width = \"400px\">\n",
    "</p>\n",
    "\n",
    "From the above chart, we can see that the food preferences Taste, Temperature and Texture are exploratory variables and Eat (Yes/No) is target variable.\n",
    "\n",
    "Now, we need to construct a top-down decision tree that splits the dataset and finally form a pure group, so we can predict for a new test variable if the kid eats or not.\n",
    "\n",
    "We are going to use the ID3 algorithm for this.\n",
    "\n",
    "<p>\n",
    "        <img src = assets/6.png/ >\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "        <img src = assets/7.png/>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "        <img src = assets/8.png/ >\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "        <img src = assets/8_1.png/ >\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "        <img src = assets/9.png/ >\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "        <img src = assets/10.png/ >\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "        <img src = assets/10_1.png/ >\n",
    "</p>\n",
    "\n",
    "\n",
    "<p>\n",
    "        <img src = assets/11.png/ >\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "        <img src = assets/12.png/ >\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "        <img src = assets/12_1.png/ >\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "        <img src = assets/13.png/ >\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "        <img src = assets/final.png/>\n",
    "</p>\n",
    "\n",
    "\n",
    "#### Conclusion:\n",
    "\n",
    "Finally, what we can conclude is, if the food is sweet the kid is not caring about its Temperature or Texture, he is eating.\n",
    "\n",
    "If the food is Salty he is eating only if the texture is hard. And if the food is Spicy he eats if it is Hot and Hard or Cold and Soft. Bizarre Kid.\n",
    "\n",
    "#### REFERENCES:\n",
    "\n",
    "- [Math behing DT](https://medium.com/@rakendd/building-decision-trees-and-its-math-711862eea1c0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code sample 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "eps = np.finfo(float).eps # ‘eps’ here is the smallest representable number. \n",
    "# At times we get log(0) or 0 in the denominator, to avoid that we are going to use this.\n",
    "\n",
    "from numpy import log2 as log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the dataset we saw above was small, we represent it with a dictionary\n",
    "\n",
    "dataset = {'Taste':['Salty','Spicy','Spicy','Spicy','Spicy','Sweet','Salty','Sweet','Spicy','Salty'],\n",
    "       'Temperature':['Hot','Hot','Hot','Cold','Hot','Cold','Cold','Hot','Cold','Hot'],\n",
    "       'Texture':['Soft','Soft','Hard','Hard','Hard','Soft','Soft','Soft','Soft','Hard'],\n",
    "       'Eat':['No','No','Yes','No','Yes','Yes','No','Yes','Yes','Yes']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dataset, columns = ['Taste', 'Temperature', 'Texture', 'Eat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Taste</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Texture</th>\n",
       "      <th>Eat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Salty</td>\n",
       "      <td>Hot</td>\n",
       "      <td>Soft</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spicy</td>\n",
       "      <td>Hot</td>\n",
       "      <td>Soft</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spicy</td>\n",
       "      <td>Hot</td>\n",
       "      <td>Hard</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spicy</td>\n",
       "      <td>Cold</td>\n",
       "      <td>Hard</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Spicy</td>\n",
       "      <td>Hot</td>\n",
       "      <td>Hard</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sweet</td>\n",
       "      <td>Cold</td>\n",
       "      <td>Soft</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Salty</td>\n",
       "      <td>Cold</td>\n",
       "      <td>Soft</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sweet</td>\n",
       "      <td>Hot</td>\n",
       "      <td>Soft</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Spicy</td>\n",
       "      <td>Cold</td>\n",
       "      <td>Soft</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Salty</td>\n",
       "      <td>Hot</td>\n",
       "      <td>Hard</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Taste Temperature Texture  Eat\n",
       "0  Salty         Hot    Soft   No\n",
       "1  Spicy         Hot    Soft   No\n",
       "2  Spicy         Hot    Hard  Yes\n",
       "3  Spicy        Cold    Hard   No\n",
       "4  Spicy         Hot    Hard  Yes\n",
       "5  Sweet        Cold    Soft  Yes\n",
       "6  Salty        Cold    Soft   No\n",
       "7  Sweet         Hot    Soft  Yes\n",
       "8  Spicy        Cold    Soft  Yes\n",
       "9  Salty         Hot    Hard  Yes"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to find entropy and then information gain for splitting the dataset\n",
    "\n",
    "<p>\n",
    "        <img src = assets/3.png/ height = \"400px\" width = \"400px\">\n",
    "</p>\n",
    "\n",
    "We’ll define a function that takes in class (target variable vector) and finds the entropy of that class.\n",
    "Here the fraction is ‘pi’, it is the proportion of a number of elements in that split group to the number of elements in the group before splitting(parent group)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_node = 0 #initialize entropy\n",
    "values = df.Eat.unique() #Unique objects - 'Yes' or 'No'\n",
    "# values\n",
    "for value in values:\n",
    "    frac = df.Eat.value_counts()[value]/len(df.Eat)\n",
    "    entropy_node += -frac*np.log2(frac) #summation\n",
    "    \n",
    "# df.Eat.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9709505944546686"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy_node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is same as the entropy (Eo) we calculated above.\n",
    "\n",
    "Now calculating the entropy of the other attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute = 'Taste'\n",
    "target_variables = df.Eat.unique() #This gives all 'Yes' and 'No'\n",
    "variables = df[attribute].unique() #This gives different features in that attribute (like 'Sweet')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Salty\n",
       "1    Spicy\n",
       "2    Spicy\n",
       "3    Spicy\n",
       "4    Spicy\n",
       "5    Sweet\n",
       "6    Salty\n",
       "7    Sweet\n",
       "8    Spicy\n",
       "9    Salty\n",
       "Name: Taste, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[attribute]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_attribute = 0\n",
    "for variable in variables:\n",
    "    entropy_each_feature = 0\n",
    "    for target_variable in target_variables:\n",
    "        num = len(df[attribute][df[attribute] == variable][df.Eat == target_variable])\n",
    "        den = len(df[attribute][df[attribute] == variable])\n",
    "        \n",
    "        frac = num/(den+eps) #pi\n",
    "        entropy_each_feature += -frac*log(frac+eps) #This calculates entropy for one feature like 'Sweet'\n",
    "        \n",
    "    frac2 = den/len(df)    \n",
    "    entropy_attribute += -frac2*entropy_each_feature #Sums up all the entropy ETaste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable: Salty\n",
      "target_variable: No\n",
      "n: 0    Salty\n",
      "6    Salty\n",
      "9    Salty\n",
      "Name: Taste, dtype: object\n",
      "num: 2 and den: 3\n",
      "entropy_each_feature: 0.3899750004807705\n",
      "target_variable: Yes\n",
      "n: 0    Salty\n",
      "6    Salty\n",
      "9    Salty\n",
      "Name: Taste, dtype: object\n",
      "num: 1 and den: 3\n",
      "entropy_each_feature: 0.9182958340544889\n",
      "frac2: 0.3\n",
      "entropy_attr: -0.27548875021634667\n",
      "variable: Spicy\n",
      "target_variable: No\n",
      "n: 1    Spicy\n",
      "2    Spicy\n",
      "3    Spicy\n",
      "4    Spicy\n",
      "8    Spicy\n",
      "Name: Taste, dtype: object\n",
      "num: 2 and den: 5\n",
      "entropy_each_feature: 0.5287712379549446\n",
      "target_variable: Yes\n",
      "n: 1    Spicy\n",
      "2    Spicy\n",
      "3    Spicy\n",
      "4    Spicy\n",
      "8    Spicy\n",
      "Name: Taste, dtype: object\n",
      "num: 3 and den: 5\n",
      "entropy_each_feature: 0.970950594454668\n",
      "frac2: 0.5\n",
      "entropy_attr: -0.7609640474436807\n",
      "variable: Sweet\n",
      "target_variable: No\n",
      "n: 5    Sweet\n",
      "7    Sweet\n",
      "Name: Taste, dtype: object\n",
      "num: 0 and den: 2\n",
      "entropy_each_feature: 0.0\n",
      "target_variable: Yes\n",
      "n: 5    Sweet\n",
      "7    Sweet\n",
      "Name: Taste, dtype: object\n",
      "num: 2 and den: 2\n",
      "entropy_each_feature: -3.2034265038149176e-16\n",
      "frac2: 0.2\n",
      "entropy_attr: -0.7609640474436806\n"
     ]
    }
   ],
   "source": [
    "entropy_attribute = 0\n",
    "for variable in variables:\n",
    "    print(f\"variable: {variable}\")\n",
    "    entropy_each_feature = 0\n",
    "    for target_variable in target_variables:\n",
    "        print(f\"target_variable: {target_variable}\")\n",
    "        print(f\"n: {df[attribute][df[attribute] == variable]}\")\n",
    "        num = len(df[attribute][df[attribute] == variable][df.Eat == target_variable])\n",
    "        den = len(df[attribute][df[attribute] == variable])\n",
    "        print(f\"num: {num} and den: {den}\")\n",
    "        frac = num/(den+eps) #pi\n",
    "        entropy_each_feature += -frac*log(frac+eps) #This calculates entropy for one feature like 'Sweet'\n",
    "        print(f\"entropy_each_feature: {entropy_each_feature}\")\n",
    "    \n",
    "    frac2 = den/len(df)    \n",
    "    print(f\"frac2: {frac2}\")\n",
    "    entropy_attribute += -frac2*entropy_each_feature #Sums up all the entropy ETaste\n",
    "    print(f\"entropy_attr: {entropy_attribute}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7609640474436806"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entropy for Taste:\n",
    "abs(entropy_attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20998654701098796"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The information gain is simply: entropy_node - entropy_attribute\n",
    "IG_taste = entropy_node - abs(entropy_attribute)\n",
    "IG_taste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now calculate the entropy for other attributes like temp and texture\n",
    "\n",
    "attribute = 'Temperature'\n",
    "target_variables = df.Eat.unique() #This gives all 'Yes' and 'No'\n",
    "variables = df[attribute].unique() #This gives different features in that attribute (like 'Sweet')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     Hot\n",
       "1     Hot\n",
       "2     Hot\n",
       "3    Cold\n",
       "4     Hot\n",
       "5    Cold\n",
       "6    Cold\n",
       "7     Hot\n",
       "8    Cold\n",
       "9     Hot\n",
       "Name: Temperature, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[attribute]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_attribute = 0\n",
    "for variable in variables:\n",
    "    entropy_each_feature = 0\n",
    "    for target_variable in target_variables:\n",
    "        num = len(df[attribute][df[attribute] == variable][df.Eat == target_variable])\n",
    "        den = len(df[attribute][df[attribute] == variable])\n",
    "        \n",
    "        frac = num/(den+eps) #pi\n",
    "        entropy_each_feature += -frac*log(frac+eps) #This calculates entropy for one feature like 'Sweet'\n",
    "        \n",
    "    frac2 = den/len(df)    \n",
    "    entropy_attribute += -frac2*entropy_each_feature #Sums up all the entropy ETaste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.950977500432693"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entropy for Temperature:\n",
    "abs(entropy_attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.019973094021975557"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The information gain is simply: entropy_node - entropy_attribute\n",
    "IG_temp = entropy_node - abs(entropy_attribute)\n",
    "IG_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute = 'Texture'\n",
    "target_variables = df.Eat.unique() #This gives all 'Yes' and 'No'\n",
    "variables = df[attribute].unique() #This gives different features in that attribute (like 'Sweet')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Soft\n",
       "1    Soft\n",
       "2    Hard\n",
       "3    Hard\n",
       "4    Hard\n",
       "5    Soft\n",
       "6    Soft\n",
       "7    Soft\n",
       "8    Soft\n",
       "9    Hard\n",
       "Name: Texture, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[attribute]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable: Soft\n",
      "target_variable: No\n",
      "n: 0    Soft\n",
      "1    Soft\n",
      "5    Soft\n",
      "6    Soft\n",
      "7    Soft\n",
      "8    Soft\n",
      "Name: Texture, dtype: object\n",
      "num: 3 and den: 6\n",
      "entropy_each_feature: 0.49999999999999967\n",
      "target_variable: Yes\n",
      "n: 0    Soft\n",
      "1    Soft\n",
      "5    Soft\n",
      "6    Soft\n",
      "7    Soft\n",
      "8    Soft\n",
      "Name: Texture, dtype: object\n",
      "num: 3 and den: 6\n",
      "entropy_each_feature: 0.9999999999999993\n",
      "frac2: 0.6\n",
      "entropy_attr: -0.5999999999999995\n",
      "variable: Hard\n",
      "target_variable: No\n",
      "n: 2    Hard\n",
      "3    Hard\n",
      "4    Hard\n",
      "9    Hard\n",
      "Name: Texture, dtype: object\n",
      "num: 1 and den: 4\n",
      "entropy_each_feature: 0.49999999999999967\n",
      "target_variable: Yes\n",
      "n: 2    Hard\n",
      "3    Hard\n",
      "4    Hard\n",
      "9    Hard\n",
      "Name: Texture, dtype: object\n",
      "num: 3 and den: 4\n",
      "entropy_each_feature: 0.8112781244591322\n",
      "frac2: 0.4\n",
      "entropy_attr: -0.9245112497836524\n"
     ]
    }
   ],
   "source": [
    "entropy_attribute = 0\n",
    "for variable in variables:\n",
    "    print(f\"variable: {variable}\")\n",
    "    entropy_each_feature = 0\n",
    "    for target_variable in target_variables:\n",
    "        print(f\"target_variable: {target_variable}\")\n",
    "        print(f\"n: {df[attribute][df[attribute] == variable]}\")\n",
    "        num = len(df[attribute][df[attribute] == variable][df.Eat == target_variable])\n",
    "        den = len(df[attribute][df[attribute] == variable])\n",
    "        print(f\"num: {num} and den: {den}\")\n",
    "        frac = num/(den+eps) #pi\n",
    "        entropy_each_feature += -frac*log(frac+eps) #This calculates entropy for one feature like 'Sweet'\n",
    "        print(f\"entropy_each_feature: {entropy_each_feature}\")\n",
    "    \n",
    "    frac2 = den/len(df)    \n",
    "    print(f\"frac2: {frac2}\")\n",
    "    entropy_attribute += -frac2*entropy_each_feature #Sums up all the entropy ETaste\n",
    "    print(f\"entropy_attr: {entropy_attribute}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9245112497836524"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entropy for Texture:\n",
    "abs(entropy_attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04643934467101618"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The information gain is simply: entropy_node - entropy_attribute\n",
    "IG_texture = entropy_node - abs(entropy_attribute)\n",
    "IG_texture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We’ll find the winner node, the one with the highest Information Gain. We repeat this process to find \n",
    "# which is the attribute we need to consider to split the data at the nodes.\n",
    "\n",
    "\n",
    "def find_entropy(df):\n",
    "    Class = df.keys()[-1]   #To make the code generic, changing target variable class name\n",
    "    entropy = 0\n",
    "    values = df[Class].unique()\n",
    "    for value in values:\n",
    "        fraction = df[Class].value_counts()[value]/len(df[Class])\n",
    "        entropy += -fraction*np.log2(fraction)\n",
    "    return entropy\n",
    "\n",
    "def find_entropy_attribute(df, attribute):\n",
    "    Class = df.keys()[-1]\n",
    "    target_variables = df[Class].unique() # Gives all 'Yes' and 'No'\n",
    "    variables = df[attribute].unique() # #This gives different features in that attribute (like 'Hot','Cold' in Temperature)\n",
    "    entropy2 = 0\n",
    "    for target_variable in target_variables:\n",
    "        entropy = 0\n",
    "        for variable in variables:\n",
    "            num = len(df[attribute][df[attribute] == variable][df[Class] == target_variable])\n",
    "            den = len(df[attribute][df[attribute] == variable])\n",
    "            \n",
    "            frac = num/(den+eps)\n",
    "            entropy += -frac*log(frac+eps)\n",
    "            \n",
    "        frac2 = den/len(df)\n",
    "        entropy2 += -frac2*entropy\n",
    "    return abs(entropy2)\n",
    "\n",
    "def find_winner(df):\n",
    "    entropy_att = []\n",
    "    IG = []\n",
    "    \n",
    "    for key in df.keys()[:-1]:\n",
    "        IG.append(find_entropy(df) - find_entropy_attribute(df, key)) \n",
    "    return df.keys()[:-1][np.argmax(IG)]\n",
    "\n",
    "def get_subtable(df, node,value):\n",
    "    return df[df[node] == value].reset_index(drop=True)\n",
    "\n",
    "def buildTree(df,tree=None): \n",
    "    Class = df.keys()[-1]   #To make the code generic, changing target variable class name\n",
    "    \n",
    "    #Here we build our decision tree\n",
    "\n",
    "    #Get attribute with maximum information gain\n",
    "    node = find_winner(df)\n",
    "\n",
    "    #Get distinct value of that attribute e.g Salary is node and Low,Med and High are values\n",
    "    attValue = np.unique(df[node])\n",
    "\n",
    "    #Create an empty dictionary to create tree    \n",
    "    if tree is None:                    \n",
    "        tree = {}\n",
    "        tree[node] = {}\n",
    "    \n",
    "    #We make loop to construct a tree by calling this function recursively. \n",
    "    #In this we check if the subset is pure and stops if it is pure. \n",
    "\n",
    "    for value in attValue:\n",
    "        subtable = get_subtable(df,node,value)\n",
    "        clValue,counts = np.unique(subtable['Eat'],return_counts=True)\n",
    "    \n",
    "        if len(counts)==1: #Checking purity of subset\n",
    "            tree[node][value] = clValue[0]                                                    \n",
    "        else:        \n",
    "            tree[node][value] = buildTree(subtable) #Calling the function recursively                \n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = buildTree(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Taste': {'Salty': {'Texture': {'Hard': 'Yes', 'Soft': 'No'}},\n",
      "           'Spicy': {'Temperature': {'Cold': {'Texture': {'Hard': 'No',\n",
      "                                                          'Soft': 'Yes'}},\n",
      "                                     'Hot': {'Texture': {'Hard': 'Yes',\n",
      "                                                         'Soft': 'No'}}}},\n",
      "           'Sweet': 'Yes'}}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "\n",
    "def predict(inst, tree):\n",
    "    \n",
    "    for nodes in tree.keys():\n",
    "        value = inst[nodes]\n",
    "        tree = tree[nodes][value]\n",
    "        \n",
    "        prediction = 0\n",
    "        \n",
    "        if type(tree) is dict:\n",
    "            prediction = predict(inst, tree)\n",
    "        else:\n",
    "            prediction = tree\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Taste          Salty\n",
       "Temperature     Cold\n",
       "Texture         Soft\n",
       "Eat               No\n",
       "Name: 6, dtype: object"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inst = df.iloc[6] # takes row with index 6\n",
    "inst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = predict(inst, tree)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on new data\n",
    "\n",
    "data = {'Taste': 'Salty', 'Temperature':'Cold', 'Texture':'Hard'}\n",
    "inst = pd.Series(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = predict(inst, tree)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code sample 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "We'll be considering the [Wireless Indoor Localization Dataset](https://archive.ics.uci.edu/ml/datasets/Wireless+Indoor+Localization) It gives 7 features representing the strength of 7 Wi-Fi signals perceived by a phone in an apartment, along with the indoor location of the phone which can be Room 1, 2, 3 or 4.\n",
    "\n",
    "The goal is to predict which room the phone is located in based on the strength of Wi-Fi signals 1 to 7. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gini impurity\n",
    "\n",
    "Decision trees use the concept of Gini impurity to describe how homogeneous or “pure” a node is. A node is pure (G = 0) if all its samples belong to the same class, while a node with many samples from many different classes will have a Gini closer to 1.\n",
    "\n",
    "<p>\n",
    "        <img src = assets/14.png/>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "        <img src = assets/15.png/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the optimal feature and threshold\n",
    "\n",
    "The key to the CART algorithm is finding the optimal feature and threshold such that the Gini impurity is minimized. To do so, we try all possible splits and compute the resulting Gini impurities.\n",
    "\n",
    "But how can we try all possible thresholds for a continuous values? There is a simple trick — sort the values for a given feature, and consider all midpoints between two adjacent values. Sorting is costly, but it is needed anyway as we will see shortly.\n",
    "\n",
    "Now, how might we compute the Gini of all possible splits?\n",
    "\n",
    "The first solution is to actually perform each split and compute the resulting Gini. Unfortunately this is slow, since we would need to look at all the samples to partition them into left and right. More precisely, it would be n splits with O(n) operations for each split, making the overall operation O(n²).\n",
    "\n",
    "A faster approach is to\n",
    "- iterate through the sorted feature values as possible thresholds \n",
    "- keep track of the number of samples per class on the left and on the right\n",
    "- increment/decrement them by 1 after each threshold. From them we can easily compute Gini in constant time.\n",
    "\n",
    "<p>\n",
    "        <img src = assets/16.png/>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "        <img src = assets/17.png/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, predicted_class):\n",
    "        self.predicted_class = predicted_class\n",
    "        self.feature_index = 0\n",
    "        self.threshold = 0\n",
    "        self.left = None\n",
    "        self.right = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier:\n",
    "    def __init__(self, max_depth = None):\n",
    "        self.max_depth = max_depth\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Build Decision Tree Classifier\n",
    "        \"\"\"\n",
    "        self.n_classes_ = len(set(y)) # classes are assumed to go from 0 to n-1\n",
    "        self.n_features_ = X.shape[1]\n",
    "        self.tree_ = self._grow_tree(X, y)\n",
    "    \n",
    "#     def _gini(self, y):\n",
    "#         \"\"\"Compute Gini impurity of a non-empty node.\n",
    "#         Gini impurity is defined as Σ p(1-p) over all classes, with p the frequency of a\n",
    "#         class within the node. Since Σ p = 1, this is equivalent to 1 - Σ p^2.\n",
    "#         \"\"\"\n",
    "#         m = y.size\n",
    "#         return 1.0 - sum((np.sum(y == c) / m) ** 2 for c in range(self.n_classes_))\n",
    "    \n",
    "    def best_split(self, X, y):\n",
    "        \"\"\"\n",
    "        Find the best split for a node.\n",
    "        \"Best\" means that the average impurity of the two children, weighted by their\n",
    "        population, is the smallest possible. Additionally it must be less than the\n",
    "        impurity of the current node.\n",
    "        To find the best split, we loop through all the features, and consider all the\n",
    "        midpoints between adjacent training samples as possible thresholds. We compute\n",
    "        the Gini impurity of the split generated by that particular feature/threshold\n",
    "        pair, and return the pair with smallest impurity.\n",
    "        Returns:\n",
    "            best_idx: Index of the feature for best split, or None if no split is found.\n",
    "            best_thr: Threshold to use for the split, or None if no split is found.\n",
    "        \"\"\"\n",
    "        # Need atleast two elements to split a node\n",
    "        m = y.size\n",
    "        \n",
    "        if m<2:\n",
    "            return None, None\n",
    "        \n",
    "        # Count of each class in current node\n",
    "        num_parent = [np.sum(y == c) for c in range(self.n_classes_)]\n",
    "        \n",
    "        # Gini of current node\n",
    "        best_gini = 1.0 - sum((n/m) ** 2 for n in num_parent)\n",
    "        best_idx, best_thr = None, None\n",
    "        \n",
    "        # Loop through all features\n",
    "        for idx in range(self.n_features_):\n",
    "            # Sort data along selected feature\n",
    "            thresholds, classes = zip(*sorted(zip(X[:, idx], y)))\n",
    "            \n",
    "            # We could actually split the node according to each feature/threshold pair\n",
    "            # and count the resulting population for each class in the children, but\n",
    "            # instead we compute them in an iterative fashion, making this for loop\n",
    "            # linear rather than quadratic.\n",
    "            \n",
    "            num_left = [0]*(self.n_classes_)\n",
    "            num_right = num_parent.copy()\n",
    "            \n",
    "            for i in range(1, m): #possible split positions\n",
    "                c = classes[i-1]\n",
    "                num_left[c] += 1\n",
    "                num_right[c] -= 1\n",
    "                \n",
    "                gini_left = 1.0 - sum((num_left[x]/i)**2 for x in range(self.n_classes_))\n",
    "                gini_right = 1.0 - sum((num_right[x]/(m-i))**2 for x in range(self.n_classes_))\n",
    "                \n",
    "                # The Gini impurity of a split is the weighted average of the Gini\n",
    "                # impurity of the children.\n",
    "                gini = (i * gini_left + (m-i) * gini_right)/m\n",
    "                \n",
    "                # The following condition is to make sure we don't try to split two\n",
    "                # points with identical values for that feature, as it is impossible\n",
    "                # (both have to end up on the same side of a split).\n",
    "                if thresholds[i] == thresholds[i-1]:\n",
    "                    continue\n",
    "                \n",
    "                if gini < best_gini:\n",
    "                    best_gini = gini\n",
    "                    best_idx = idx\n",
    "                    best_thr = (thresholds[i] + thresholds[i-1])/2 # midpoint\n",
    "            return best_idx, best_thr\n",
    "    \n",
    "    def _grow_tree(self, X, y, depth = 0):\n",
    "        \"\"\"\n",
    "        Build DT by recursively finding the best split\n",
    "        \"\"\"\n",
    "        # Population for each class in current node. The predicted class is the one with\n",
    "        # largest population.\n",
    "        \n",
    "        num_samples_per_class = np.sum((y==i) for i in range(self.n_classes_))\n",
    "        predicted_class = np.argmax(num_samples_per_class)\n",
    "        node = Node(predicted_class = predicted_class)\n",
    "        \n",
    "        # Split recursively until max_depth is reached\n",
    "        if depth < self.max_depth:\n",
    "            idx, thr = self.best_split(X, y)\n",
    "            if idx is not None:\n",
    "                indices_left = X[:, idx] < thr\n",
    "                X_left, y_left = X[indices_left], y[indices_left]\n",
    "                X_right, y_right = X[~indices_left], y[~indices_left]\n",
    "                node.feature_index = idx\n",
    "                node.threshold = thr\n",
    "                node.left = self._grow_tree(X_left, y_left, depth + 1)\n",
    "                node.right = self._grow_tree(X_right, y_right, depth + 1)\n",
    "        return node\n",
    "    \n",
    "    # Making predictions\n",
    "    # Go left if the feature value is below the threshold, go right otherwise.\n",
    "    def predict(self, X):\n",
    "        return [self._predict(inputs) for inputs in X]\n",
    "    \n",
    "    def _predict(self, inputs):\n",
    "        \"\"\"\n",
    "        Predict class for a single sample\n",
    "        \"\"\"\n",
    "        node = self.tree_\n",
    "        while node.left:\n",
    "            if inputs[node.feature_index] < node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "            return node.predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [-70, 0, 0, 0, -40, 0, 0]\n",
      "Prediction: Room 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhishek/anaconda3/envs/tf2/lib/python3.7/site-packages/ipykernel_launcher.py:92: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import Bunch\n",
    "\n",
    "df = pd.read_csv(\"wifi_localization.txt\", delimiter=\"\\t\")\n",
    "data = df.to_numpy()\n",
    "\n",
    "dataset = Bunch(\n",
    "            data=data[:, :-1],\n",
    "            target=data[:, -1] - 1,\n",
    "            feature_names=[\"Wifi {}\".format(i) for i in range(1, 8)],\n",
    "            target_names=[\"Room {}\".format(i) for i in range(1, 5)],\n",
    "        )\n",
    "\n",
    "X, y = dataset.data, dataset.target\n",
    "    \n",
    "# Fit data.\n",
    "clf = DecisionTreeClassifier(max_depth=2)\n",
    "clf.fit(X, y)\n",
    "\n",
    "input = [-70, 0, 0, 0, -40, 0, 0]\n",
    "\n",
    "pred = clf.predict([input])[0]\n",
    "print(\"Input: {}\".format(input))\n",
    "print(\"Prediction: \" + dataset.target_names[pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhishek/anaconda3/envs/tf2/lib/python3.7/site-packages/ipykernel_launcher.py:92: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "dataset = load_iris()\n",
    "X, y = dataset.data, dataset.target  # pylint: disable=no-member\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=1)\n",
    "clf.fit(X, y)\n",
    "print(clf.predict([[0, 0, 5, 1.5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "        <img src = assets/18.png/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complexity\n",
    "\n",
    "It’s easy to see that prediction is in O(log m), where m is the depth of the tree.\n",
    "But how about training? The Master Theorem will be helpful here. The time complexity of fitting a tree on a dataset with n samples can be expressed with the following recurrence relation:\n",
    "\n",
    "<p>\n",
    "        <img src = assets/19.png/>\n",
    "</p>\n",
    "\n",
    "where, assuming the best case where left and right children have the same size, a = 2 and b = 2; and f(n) is the complexity of splitting the node in two children, in other words the complexity of _best_split. The first for loop iterates on the features, and for each iteration there is a sort of complexity O(n log n) and another for loop in O(n). Therefore f(n) is O(k n log n) where k is the number of features.\n",
    "\n",
    "With those assumptions, the Master Theorem tells us that the total time complexity is\n",
    "\n",
    "<p>\n",
    "        <img src = assets/20.png/>\n",
    "</p>\n",
    "\n",
    "This is not too far from but still worse than the complexity of the Scikit-Learn implementation, apparently in O(k n log n). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REFERENCES:\n",
    "- [DT from scratch python](https://towardsdatascience.com/decision-tree-from-scratch-in-python-46e99dfea775)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
