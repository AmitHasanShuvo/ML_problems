{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a decision tree?\n",
    "\n",
    "Decision trees can be used for regression (continuous real-valued output, e.g. predicting the price of a house) or classification (categorical output, e.g. predicting email spam vs. no spam), but here we will focus on classification.\n",
    "\n",
    "A decision tree classifier is a binary tree where predictions are made by traversing the tree from root to leaf — at each node, we go left if a feature is less than a threshold, right otherwise. Finally, each leaf is associated with a class, which is the output of the predictor.\n",
    "\n",
    "### Their types:\n",
    "\n",
    "There is a wide variety of decision trees, such as:\n",
    "- ID3 (Iterative Dichotomiser 3)\n",
    "- CART (Classification and Regression Tree)\n",
    "- CHAID (Chi-squared Automatic Interaction Detector), etc.\n",
    "\n",
    "#### How does it work?\n",
    "\n",
    "- Starts at the root node\n",
    "- Splits data into groups (based on some criteria)\n",
    "- Set a decision at node\n",
    "- Move the data along the respective branches\n",
    "- Repeat the process until a stopping criterion is met (max levels/depth reached, min samples left to split, nothing left to split, etc)\n",
    "\n",
    "#### How to choose root node:\n",
    "\n",
    "This is slightly different in regression and classification trees.\n",
    "\n",
    "**In regression trees**, we chose a splitting point such that there is the greatest reduction in RSS (Residual Sum of Squares).\n",
    "\n",
    "\n",
    "<p>\n",
    "        <img src = assets/1.png/ height = \"400px\" width = \"400px\">\n",
    "</p>\n",
    "\n",
    "Or we can calculate standard deviation reduction of the feature with respect to the training data. Here YR1 and YR2 are mean responses of region 1 and 2. Once we train the tree, we predict the response for a test data using the mean of the training observations in that group.\n",
    "\n",
    "**In Classification trees:**\n",
    "\n",
    "We use Entropy and Information Gain (in ID3). Gini Index for classification in the CART.\n",
    "\n",
    "**Entropy (Shannon’s Entropy)** quantifies the uncertainty of chaos in the group. Higher entropy means higher the disorder. It is denoted by H(x), where x is a vector with probabilities p1, p2, p3…..\n",
    "\n",
    "<p>\n",
    "        <img src = assets/2.png/ height = \"200px\" width = \"200px\">\n",
    "</p>\n",
    "\n",
    "From the above figure, we can see that the entropy (uncertainty) is highest (1) when the probability is 0.5, i.e. 50–50 chances. And entropy is lowest when the probability is 0 or 1, i.e. there is no uncertainty or high chance of occurrence.\n",
    "\n",
    "\n",
    "So, entropy is maximum if in a class there are an equal number of objects from different attributes (like the group has 50 cats and 50 dogs), and this is minimum if the node is pure (like the group has only 100 cats or only 100 dogs). We ultimately want to have minimum entropy for the tree, i.e. pure or uniform classes at the leaf nodes.\n",
    "\n",
    "##### Entropy calculation:\n",
    "\n",
    "<p>\n",
    "        <img src = assets/3.png/ height = \"400px\" width = \"400px\">\n",
    "</p>\n",
    "\n",
    "- S — Current group for which we are interested in calculating entropy.\n",
    "- Pi — Probability of finding that system in ith state, or this turns to the proportion of a number of elements in that split group to the number of elements in the group before splitting(parent group).\n",
    "\n",
    "In this classification tree, while splitting the tree we select those attributes that achieves the greatest reduction in entropy. Now, this reduction (or change) in entropy is measured by **Information Gain** which is given by:\n",
    "\n",
    "<p>\n",
    "        <img src = assets/4.png/ height = \"400px\" width = \"400px\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:\n",
    "\n",
    "The problem is about predicting whether some kid is going to eat a particular type of food given that kids prior eating habits.\n",
    "\n",
    "<p>\n",
    "        <img src = assets/5.png/ height = \"400px\" width = \"400px\">\n",
    "</p>\n",
    "\n",
    "From the above chart, we can see that the food preferences Taste, Temperature and Texture are exploratory variables and Eat (Yes/No) is target variable.\n",
    "\n",
    "Now, we need to construct a top-down decision tree that splits the dataset and finally form a pure group, so we can predict for a new test variable if the kid eats or not.\n",
    "\n",
    "We are going to use the ID3 algorithm for this.\n",
    "\n",
    "<p>\n",
    "        <img src = assets/6.png/ >\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "        <img src = assets/7.png/>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "        <img src = assets/8.png/ >\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "        <img src = assets/8_1.png/ >\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "        <img src = assets/9.png/ >\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "        <img src = assets/10.png/ >\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "        <img src = assets/10_1.png/ >\n",
    "</p>\n",
    "\n",
    "\n",
    "<p>\n",
    "        <img src = assets/11.png/ >\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "        <img src = assets/12.png/ >\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "        <img src = assets/12_1.png/ >\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "        <img src = assets/13.png/ >\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "        <img src = assets/final.png/>\n",
    "</p>\n",
    "\n",
    "\n",
    "#### Conclusion:\n",
    "\n",
    "Finally, what we can conclude is, if the food is sweet the kid is not caring about its Temperature or Texture, he is eating.\n",
    "\n",
    "If the food is Salty he is eating only if the texture is hard. And if the food is Spicy he eats if it is Hot and Hard or Cold and Soft. Bizarre Kid.\n",
    "\n",
    "#### REFERENCES:\n",
    "\n",
    "- [Math behing DT](https://medium.com/@rakendd/building-decision-trees-and-its-math-711862eea1c0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code sample 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "eps = np.finfo(float).eps # ‘eps’ here is the smallest representable number. \n",
    "# At times we get log(0) or 0 in the denominator, to avoid that we are going to use this.\n",
    "\n",
    "from numpy import log2 as log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the dataset we saw above was small, we represent it with a dictionary\n",
    "\n",
    "dataset = {'Taste':['Salty','Spicy','Spicy','Spicy','Spicy','Sweet','Salty','Sweet','Spicy','Salty'],\n",
    "       'Temperature':['Hot','Hot','Hot','Cold','Hot','Cold','Cold','Hot','Cold','Hot'],\n",
    "       'Texture':['Soft','Soft','Hard','Hard','Hard','Soft','Soft','Soft','Soft','Hard'],\n",
    "       'Eat':['No','No','Yes','No','Yes','Yes','No','Yes','Yes','Yes']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dataset, columns = ['Taste', 'Temperature', 'Texture', 'Eat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Taste</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Texture</th>\n",
       "      <th>Eat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Salty</td>\n",
       "      <td>Hot</td>\n",
       "      <td>Soft</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spicy</td>\n",
       "      <td>Hot</td>\n",
       "      <td>Soft</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spicy</td>\n",
       "      <td>Hot</td>\n",
       "      <td>Hard</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spicy</td>\n",
       "      <td>Cold</td>\n",
       "      <td>Hard</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Spicy</td>\n",
       "      <td>Hot</td>\n",
       "      <td>Hard</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sweet</td>\n",
       "      <td>Cold</td>\n",
       "      <td>Soft</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Salty</td>\n",
       "      <td>Cold</td>\n",
       "      <td>Soft</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sweet</td>\n",
       "      <td>Hot</td>\n",
       "      <td>Soft</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Spicy</td>\n",
       "      <td>Cold</td>\n",
       "      <td>Soft</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Salty</td>\n",
       "      <td>Hot</td>\n",
       "      <td>Hard</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Taste Temperature Texture  Eat\n",
       "0  Salty         Hot    Soft   No\n",
       "1  Spicy         Hot    Soft   No\n",
       "2  Spicy         Hot    Hard  Yes\n",
       "3  Spicy        Cold    Hard   No\n",
       "4  Spicy         Hot    Hard  Yes\n",
       "5  Sweet        Cold    Soft  Yes\n",
       "6  Salty        Cold    Soft   No\n",
       "7  Sweet         Hot    Soft  Yes\n",
       "8  Spicy        Cold    Soft  Yes\n",
       "9  Salty         Hot    Hard  Yes"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to find entropy and then information gain for splitting the dataset\n",
    "\n",
    "<p>\n",
    "        <img src = assets/3.png/ height = \"400px\" width = \"400px\">\n",
    "</p>\n",
    "\n",
    "We’ll define a function that takes in class (target variable vector) and finds the entropy of that class.\n",
    "Here the fraction is ‘pi’, it is the proportion of a number of elements in that split group to the number of elements in the group before splitting(parent group)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_node = 0 #initialize entropy\n",
    "values = df.Eat.unique() #Unique objects - 'Yes' or 'No'\n",
    "# values\n",
    "for value in values:\n",
    "    frac = df.Eat.value_counts()[value]/len(df.Eat)\n",
    "    entropy_node += -frac*np.log2(frac) #summation\n",
    "    \n",
    "# df.Eat.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9709505944546686"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy_node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is same as the entropy (Eo) we calculated above.\n",
    "\n",
    "Now calculating the entropy of the other attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute = 'Taste'\n",
    "target_variables = df.Eat.unique() #This gives all 'Yes' and 'No'\n",
    "variables = df[attribute].unique() #This gives different features in that attribute (like 'Sweet')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Salty\n",
       "1    Spicy\n",
       "2    Spicy\n",
       "3    Spicy\n",
       "4    Spicy\n",
       "5    Sweet\n",
       "6    Salty\n",
       "7    Sweet\n",
       "8    Spicy\n",
       "9    Salty\n",
       "Name: Taste, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[attribute]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_attribute = 0\n",
    "for variable in variables:\n",
    "    entropy_each_feature = 0\n",
    "    for target_variable in target_variables:\n",
    "        num = len(df[attribute][df[attribute] == variable][df.Eat == target_variable])\n",
    "        den = len(df[attribute][df[attribute] == variable])\n",
    "        \n",
    "        frac = num/(den+eps) #pi\n",
    "        entropy_each_feature += -frac*log(frac+eps) #This calculates entropy for one feature like 'Sweet'\n",
    "        \n",
    "    frac2 = den/len(df)    \n",
    "    entropy_attribute += -frac2*entropy_each_feature #Sums up all the entropy ETaste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7609640474436806"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entropy for Taste:\n",
    "abs(entropy_attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20998654701098796"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The information gain is simply: entropy_node - entropy_attribute\n",
    "IG_taste = entropy_node - abs(entropy_attribute)\n",
    "IG_taste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now calculate the entropy for other attributes like temp and texture\n",
    "\n",
    "attribute = 'Temperature'\n",
    "target_variables = df.Eat.unique() #This gives all 'Yes' and 'No'\n",
    "variables = df[attribute].unique() #This gives different features in that attribute (like 'Sweet')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     Hot\n",
       "1     Hot\n",
       "2     Hot\n",
       "3    Cold\n",
       "4     Hot\n",
       "5    Cold\n",
       "6    Cold\n",
       "7     Hot\n",
       "8    Cold\n",
       "9     Hot\n",
       "Name: Temperature, dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[attribute]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_attribute = 0\n",
    "for variable in variables:\n",
    "    entropy_each_feature = 0\n",
    "    for target_variable in target_variables:\n",
    "        num = len(df[attribute][df[attribute] == variable][df.Eat == target_variable])\n",
    "        den = len(df[attribute][df[attribute] == variable])\n",
    "        \n",
    "        frac = num/(den+eps) #pi\n",
    "        entropy_each_feature += -frac*log(frac+eps) #This calculates entropy for one feature like 'Sweet'\n",
    "        \n",
    "    frac2 = den/len(df)    \n",
    "    entropy_attribute += -frac2*entropy_each_feature #Sums up all the entropy ETaste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.950977500432693"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entropy for Temperature:\n",
    "abs(entropy_attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.019973094021975557"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The information gain is simply: entropy_node - entropy_attribute\n",
    "IG_temp = entropy_node - abs(entropy_attribute)\n",
    "IG_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute = 'Texture'\n",
    "target_variables = df.Eat.unique() #This gives all 'Yes' and 'No'\n",
    "variables = df[attribute].unique() #This gives different features in that attribute (like 'Sweet')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Soft\n",
       "1    Soft\n",
       "2    Hard\n",
       "3    Hard\n",
       "4    Hard\n",
       "5    Soft\n",
       "6    Soft\n",
       "7    Soft\n",
       "8    Soft\n",
       "9    Hard\n",
       "Name: Texture, dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[attribute]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_attribute = 0\n",
    "for variable in variables:\n",
    "    entropy_each_feature = 0\n",
    "    for target_variable in target_variables:\n",
    "        num = len(df[attribute][df[attribute] == variable][df.Eat == target_variable])\n",
    "        den = len(df[attribute][df[attribute] == variable])\n",
    "        \n",
    "        frac = num/(den+eps) #pi\n",
    "        entropy_each_feature += -frac*log(frac+eps) #This calculates entropy for one feature like 'Sweet'\n",
    "        \n",
    "    frac2 = den/len(df)    \n",
    "    entropy_attribute += -frac2*entropy_each_feature #Sums up all the entropy ETaste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9245112497836524"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entropy for Texture:\n",
    "abs(entropy_attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04643934467101618"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The information gain is simply: entropy_node - entropy_attribute\n",
    "IG_texture = entropy_node - abs(entropy_attribute)\n",
    "IG_texture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We’ll find the winner node, the one with the highest Information Gain. We repeat this process to find \n",
    "# which is the attribute we need to consider to split the data at the nodes.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
