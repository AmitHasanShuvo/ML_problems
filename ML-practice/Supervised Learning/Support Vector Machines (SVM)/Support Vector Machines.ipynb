{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines:\n",
    "\n",
    "Support vector machines focus only on the points that are the most difficult to tell apart, whereas other classifiers pay attention to all of the points. \n",
    "\n",
    "The intuition behind the support vector machine approach is that if a classifier is good at the most challenging comparisons (the points in B and A that are closest to each other in Figure 2), then the classifier will be even better at the easy comparisons (comparing points in B and A that are far away from each other). \n",
    "\n",
    "**Perceptrons and other classifiers:**\n",
    "\n",
    "Perceptrons are built by taking one point at a time and adjusting the dividing line accordingly. As soon as all of the points are separated, the perceptron algorithm stops. But it could stop anywhere. Figure 1 shows that there are a bunch of different dividing lines that separate the data. The perceptron's stopping criteria is simple: \"separate the points and stop improving the line when you get 100% separation\". The perceptron is not explicitly told to find the best separating line. Logistic regression and linear discriminant models are built similarly to perceptrons. \n",
    "\n",
    "The best dividing line maximizes the distance between the B points closest to A and the A points closest to B. It's not necessary to look at all of the points to do this. In fact, incorporating feedback from points that are far away can bump the line a little too far, as seen below. \n",
    "\n",
    "<p>\n",
    "        <img src = assets/1.png/>\n",
    "</p>\n",
    "\n",
    "**Support Vector Machines:**\n",
    "\n",
    "Unlike other classifiers, the support vector machine is ***explicitly*** told to find the best separating line. How? The support vector machine searches for the closest points (Figure 2), which it calls the \"support vectors\" (the name \"support vector machine\" is due to the fact that points are like vectors and that the best line \"depends on\" or is \"supported by\" the closest points). \n",
    "\n",
    "Once it has found the closest points, the SVM draws a line connecting them (see the line labeled 'w' in Figure 2). It draws this connecting line by doing vector subtraction (point A - point B). The support vector machine then declares the best separating line to be the line that bisects -- and is perpendicular to -- the connecting line.  \n",
    "\n",
    "The support vector machine is better because when you get a new sample (new points), you will have already made a line that keeps B and A as far away from each other as possible, and so it is less likely that one will spillover across the line into the other's territory. \n",
    "\n",
    "<p>\n",
    "        <img src = assets/2.png/>\n",
    "</p>\n",
    "\n",
    "The paper called *[Duality and Geometry in SVM Classifiers](http://www.robots.ox.ac.uk/~cvrg/bennett00duality.pdf)* finally helped me see the light; that's where I got the images from. \n",
    "\n",
    "\n",
    "## Mathematical overview of how SVM's are trained and used:\n",
    "\n",
    "\n",
    "## Notations ##\n",
    "\n",
    "In the following, scalars are denoted with italic lowercases (e.g., $y,\\, b$), vectors with bold lowercases (e.g., $\\mathbf{w},\\, \\mathbf{x}$), and matrices with italic uppercases (e.g., $W$). $\\mathbf{w^T}$ is the transpose of $\\mathbf{w}$, and $\\|\\mathbf{w}\\| = \\mathbf{w}^T\\mathbf{w}$.\n",
    "\n",
    "Let:\n",
    "\n",
    "- $\\mathbf{x}$ be a feature vector (i.e., the input of the SVM). $\\mathbf{x} \\in \\mathbb{R}^n$, where $n$ is the dimension of the feature vector.\n",
    "- $y$ be the class (i.e., the output of the SVM). $y \\in \\{ -1,1\\}$, i.e. the classification task is binary. \n",
    "- $\\mathbf{w}$ and $b$ be the parameters of the SVM: we need to learn them using the training set.\n",
    "- $(\\mathbf{x}^{(i)}, y^{(i)})$ be the $i^ {\\text {th}}$ sample in the dataset. Let's assume we have $N$ samples in the training set.\n",
    "\n",
    "With $n=2$, one can represent the SVM's decision boundaries as follows:\n",
    "\n",
    "<p>\n",
    "        <img src = assets/3.png/ width = 400px height = 400px>\n",
    "</p>\n",
    "\n",
    "The class $y$ is determined as follows:\n",
    "\n",
    "$$ \n",
    "y^{(i)}=\\left\\{\n",
    "                \\begin{array}{ll}\n",
    "                  -1 &\\text{ if } \\mathbf{w^T}\\mathbf{x}^{(i)}+b \\leq -1 \\\\\n",
    "                  1 &\\text{ if } \\mathbf{w^T}\\mathbf{x}^{(i)}+b \\ge 1 \\\\\n",
    "                \\end{array}\n",
    "              \\right.\n",
    "$$\n",
    "\n",
    "which can be more concisely written as $y^{(i)} (\\mathbf{w^T}\\mathbf{x}^{(i)}+b) \\ge 1$.\n",
    "\n",
    "## Goal  ##\n",
    "\n",
    "The SVM aims at satisfying two requirements:\n",
    "\n",
    "1. The SVM should maximize the distance between the two decision boundaries. Mathematically, this means we want to maximize the distance between the hyperplane defined by $\\mathbf{w^T}\\mathbf{x}+b = -1$ and the hyperplane defined by $\\mathbf{w^T}\\mathbf{x}+b = 1$. [This distance is equal to $\\frac{2}{\\|\\mathbf{w}\\|}$](https://math.stackexchange.com/a/1306417/168764). This means we want to solve $\\underset{\\mathbf{w}}{\\operatorname{max}} \\frac{2}{\\|\\mathbf{w}\\|}$. Equivalently we want\n",
    "$\\underset{\\mathbf{w}}{\\operatorname{min}} \\frac{\\|\\mathbf{w}\\|}{2}$.\n",
    "\n",
    "2. The SVM should also correctly classify all $\\mathbf{x}^{(i)}$, which means $y^{(i)} (\\mathbf{w^T}\\mathbf{x}^{(i)}+b) \\ge 1, \\forall i \\in \\{1,\\dots,N\\}$\n",
    "\n",
    "\n",
    "Which leads us to the following quadratic optimization problem:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\min_{\\mathbf{w},b}\\quad &\\frac{\\|\\mathbf{w}\\|}{2}, \\\\\n",
    "s.t.\\quad&y^{(i)} (\\mathbf{w^T}\\mathbf{x}^{(i)}+b) \\ge 1 &\\forall i \\in \\{1,\\dots,N\\}\n",
    "\\end{align}$$\n",
    "\n",
    "This is the **hard-margin SVM**, as this quadratic optimization problem admits a solution iff the data is linearly separable. \n",
    "\n",
    "One can relax the constraints by introducing so-called **slack variables** $\\xi^{(i)}$. Note that each sample of the training set has its own slack variable. This gives us the following quadratic optimization problem:\n",
    "\n",
    "\n",
    "$$\\begin{align}\n",
    "\\min_{\\mathbf{w},b}\\quad &\\frac{\\|\\mathbf{w}\\|}{2}+ C \\sum_{i=1}^{N} \\xi^{(i)}, \\\\\n",
    "s.t.\\quad&y^{(i)} (\\mathbf{w^T}\\mathbf{x}^{(i)}+b) \\ge 1 - \\xi^{(i)},&\\forall i \\in \\{1,\\dots,N\\} \\\\\n",
    "\\quad&\\xi^{(i)}\\ge0, &\\forall i \\in \\{1,\\dots,N\\} \n",
    "\\end{align}$$\n",
    "\n",
    "This is the **soft-margin SVM**. $C$ is a hyperparameter called **penalty of the error term**.\n",
    " ([What is the influence of C in SVMs with linear kernel?](https://stats.stackexchange.com/q/31066/12359) and [Which search range for determining SVM optimal parameters?](https://stats.stackexchange.com/q/43943/12359)).\n",
    "\n",
    "One can add even more flexibility by introducing a function $\\phi$ that maps the original feature space to a higher dimensional feature space. This allows non-linear decision boundaries. The quadratic optimization problem becomes:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\min_{\\mathbf{w},b}\\quad &\\frac{\\|\\mathbf{w}\\|}{2}+ C \\sum_{i=1}^{N} \\xi^{(i)}, \\\\\n",
    "s.t.\\quad&y^{(i)} (\\mathbf{w^T}\\phi \\left(\\mathbf{x}^{(i)}\\right)+b) \\ge 1 - \\xi^{(i)},&\\forall i \\in \\{1,\\dots,N\\} \\\\\n",
    "\\quad&\\xi^{(i)}\\ge0, &\\forall i \\in \\{1,\\dots,N\\} \n",
    "\\end{align}$$\n",
    "\n",
    "\n",
    "## Optimization  ##\n",
    "\n",
    "\n",
    "### Solving using the Lagrangian:\n",
    "\n",
    "<p>\n",
    "        <img src = assets/4.png/ width = 400px height = 400px>\n",
    "</p>\n",
    "\n",
    "\n",
    "<p>\n",
    "        <img src = assets/5.png/ width = 600px height = 600px>\n",
    "</p>\n",
    "\n",
    "### Hence, the optimization depends on the dot product of pairs of examples\n",
    "--------------------------------------------------------------------------------------------\n",
    "-------------------\n",
    "The quadratic optimization problem can be transformed into another optimization problem named the [**Lagrangian dual problem**](https://en.wikipedia.org/w/index.php?title=Duality_(optimization)&oldid=755311099#Convex_problems) (the previous problem is called the **primal**):\n",
    "\n",
    "$$\\begin{align}\n",
    "\\max_{\\mathbf{\\alpha}}  \\quad &\\min_{\\mathbf{w},b}  \\frac{\\|\\mathbf{w}\\|}{2}+ C \\sum_{i=1}^{N} \\alpha^{(i)} \\left(1-\\mathbf{w^T}\\phi \\left(\\mathbf{x}^{(i)}\\right)+b)\\right), \\\\\n",
    "s.t. \\quad&0 \\leq  \\alpha^{(i)} \\leq C, &\\forall i \\in \\{1,\\dots,N\\} \n",
    "\\end{align}$$\n",
    "\n",
    "This optimization problem can be simplified (by setting some gradients to $0$) to:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\max_{\\mathbf{\\alpha}}  \\quad & \\sum_{i=1}^{N} \\alpha^{(i)} - \\sum_{i=1}^{N}\\sum_{j=1}^{N}  \\left( y^{(i)}\\alpha^{(i)}\\phi\\left(\\mathbf{x}^{(i)}\\right)^T \\phi\\left(\\mathbf{x}^{(j)}\\right) y^{(j)}\\alpha^{(j)} \\right), \\\\\n",
    "s.t. \\quad&0 \\leq  \\alpha^{(i)} \\leq C, &\\forall i \\in \\{1,\\dots,N\\}  \n",
    "\\end{align}$$\n",
    "\n",
    "$\\mathbf{w}$ doesn't appear as $\\mathbf{w}=\\sum_{i =1}^{N}\\alpha^{(i)}y^{(i)}\\phi\\left(x^{(i)}\\right)$ (as stated by the [representer theorem](https://en.wikipedia.org/wiki/Representer_theorem)).\n",
    "\n",
    "We therefore learn the $\\alpha^{(i)}$ using the $(\\mathbf{x}^{(i)}, y^{(i)})$ of the training set.\n",
    "\n",
    "(FYI: [Why bother with the dual problem when fitting SVM?](https://stats.stackexchange.com/q/19181/12359) short answer: faster computation + allows to use the kernel trick, though there exist some good methods to train SVM in the primal e.g. see  {1})\n",
    "\n",
    "## Making a prediction  ##\n",
    "\n",
    "Once the $\\alpha^{(i)}$ are learned, one can predict the class of a new sample with the feature vector $\\mathbf{x}^{\\text {test}}$ as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "y^{\\text {test}}&=\\text {sign}\\left(\\mathbf{w^T}\\phi\\left(\\mathbf{x}^{\\text {test}}\\right)+b\\right) \\\\\n",
    "&= \\text {sign}\\left(\\sum_{i =1}^{N}\\alpha^{(i)}y^{(i)}\\phi\\left(x^{(i)}\\right)^T\\phi\\left(\\mathbf{x}^{\\text {test}}\\right)+b \\right)\n",
    "\\end{align*}\n",
    "\n",
    "The summation $\\sum_{i =1}^{N}$ could seem overwhelming, since it means one has to sum over all the training samples, but the vast majority of $\\alpha^{(i)}$ are $0$ (see [Why are the Lagrange multipliers sparse for SVMs?](https://stats.stackexchange.com/q/54976/12359)) so in practice it isn't an issue. (note that [one can construct special cases where all $\\alpha^{{(i)}} > 0$](https://stats.stackexchange.com/q/110598/12359).) $\\alpha^{{(i)}}=0$ iff $x^{{(i)}}$ is a **support vector**. The illustration above has 3 support vectors.\n",
    "\n",
    "# Some comments on Duality and KTT conditions\n",
    "### Primal problem\n",
    "\n",
    "Picking up from @Antoni's post in between equations $(4)$ and $(5)$, recall that our original, or **primal**, optimization problem is of the form: \n",
    "\n",
    "\n",
    "\\begin{aligned}\n",
    "\t\\min_{w, b} f(w,b) & = \\min_{w, b}  \\  \\frac{1}{2} ||w||^2\n",
    "\t\\\\\n",
    "\ts.t. \\ \\  g_i(w,b) &= - y^{(i)} (w^T x^{(i)} + b) + 1 = 0 \n",
    "\\end{aligned}\n",
    "\n",
    "\n",
    "### Lagrange method\n",
    "The method of Lagrange multipliers allows us to turn a constrained optimization problem into an unconstrained one of the form:\n",
    "\n",
    "$$\\mathcal{L}(w, b, \\alpha) =   \\frac{1}{2} ||w||^2 - \\sum_i^m \\alpha_i [y^{(i)} (w^T x^{(i)} + b) - 1]$$\n",
    "\n",
    "Where  $\\mathcal{L}(w, b, \\alpha)$ is called the **Lagrangian** and $\\alpha_i$ are called the **Lagrangian multipliers**.  \n",
    "\n",
    "Our **primal** optimization problem with the Lagrangian becomes the following: (note that the use of $min$, $max$ is not the most rigorous as we should also be using $\\inf$ and $\\sup$ here...)\n",
    "\n",
    "$$ \\min_{w,b} \\left( \\max_\\alpha \\mathcal{L}(w, b, \\alpha)\\right)$$\n",
    "\n",
    "### Dual problem\n",
    "What @Antoni and Prof. Patrick Winston have done in their derivation is assume that the optimization function and the constraints meet some technical conditions such that we can do the following: \n",
    "\n",
    "$$ \\min_{w,b} \\left( \\max_\\alpha \\mathcal{L}(w, b, \\alpha)\\right) =   \\max_\\alpha \\left( \\min_{w,b} \\mathcal{L}(w, b, \\alpha)\\right)$$\n",
    "\n",
    "This allows us to take the partial derivatives of $\\mathcal{L}(w, b, \\alpha)$ with respect to $w$ and $b$, equate to zero and then plug the results back into the original equation of the Lagrangian, hence generating an equivalent **dual** optimization problem of the form\n",
    "\n",
    "\n",
    "\\begin{aligned}\n",
    "\t&\\max_{\\alpha} \\min_{w,b} \\mathcal{L}(w,b,\\alpha)\n",
    "\t\\\\\n",
    "\t& \\max_{\\alpha} \\sum_i^m \\alpha_i - \\frac{1}{2} \\sum_{i,j}^m y^{(i)}y^{(j)} \\alpha_i \\alpha_j <x^{(i)} x^{(j)}> \n",
    "\t\\\\\n",
    "\t& s.t. \\ \\alpha_i \\geq 0 \n",
    "\t\\\\\n",
    "\t& s.t. \\ \\sum_i^m \\alpha_i y^{(i)} = 0\t\n",
    "\\end{aligned}\n",
    " \n",
    "### Duality and KTT\n",
    "Without going into excessive mathematical technicalities, these conditions are a combination of the Duality and the Karush Kuhn Tucker (KTT) conditions and allow us to solve the **dual** problem instead of the **primal** one, while ensuring that the optimal solution is the same. In our case the conditions are the following: \n",
    "\n",
    "- The primal objective and inequality constraint functions must be convex\n",
    "- The equality constraint function must be affine\n",
    "- The constraints must be strictly feasible\n",
    "\n",
    "\n",
    "Then there exists $w^*, \\alpha^*$ which are solutions to the primal and dual problems. Moreover, the parameters $w^*, \\alpha^*$ satisfy the KTT conditions below:\n",
    "\n",
    " \n",
    "\\begin{aligned}\n",
    "\t&\\frac{\\partial}{\\partial w_i}  \\mathcal{L}(w^*, \\alpha^*, \\beta^*) = 0 &(A)\n",
    "\t\\\\\n",
    "\t&\\frac{\\partial}{\\partial \\beta_i}  \\mathcal{L}(w^*, \\alpha^*, \\beta^*) = 0 &(B)\n",
    "\t\\\\\n",
    "\t&\\alpha_i^* g_i(w^*) = 0 &(C)\n",
    "\t\\\\\n",
    "\t&g_i(w^*) \\leq 0  &(D)\n",
    "\t\\\\\n",
    "\t&\\alpha_i^* \\geq 0 &(E)\n",
    "\\end{aligned}\n",
    "\n",
    "\n",
    "Moreover, if some $w^*, \\alpha^*$ satisfy the KTT solutions then they are also solution to the primal and dual problem. \n",
    "\n",
    "\n",
    "\n",
    "Equation $(C)$ above is of particular importance and is called the *dual complementarity condition*. It implies that if $\\alpha_i^* > 0$ then $g_i(w^*) = 0$ which means that the constraint $g_i(w) \\leq 0$ is active, i.e. it holds with equality rather than inequality. This is the explanation behind equation $(2)$ in Antoni's derivation where the inequality constraint is turned into an equality constraint. \n",
    "\n",
    "### A intuitive but informal diagram\n",
    "\n",
    "<p>\n",
    "        <img src = assets/6.png/>\n",
    "</p>\n",
    "\n",
    "----------\n",
    "---------\n",
    "## Kernel trick  ##\n",
    "\n",
    "One can observe that the optimization problem uses the $\\phi\\left(\\mathbf{x}^{(i)}\\right)$ only in the inner product $\\phi\\left(\\mathbf{x}^{(i)}\\right)^T \\phi\\left(\\mathbf{x}^{(j)}\\right)$. The function that maps $\\left(\\mathbf{x}^{(i)},\\mathbf{x}^{(j)}\\right)$ to the inner product $\\phi\\left(\\mathbf{x}^{(i)}\\right)^T \\phi\\left(\\mathbf{x}^{(j)}\\right)$ is [called](https://stats.stackexchange.com/a/226196/12359) a **kernel**, a.k.a. kernel function, often denoted by $k$.\n",
    "\n",
    "One can choose $k$ so that the inner product is efficient to compute. This allows to use a potentially high feature space at a low computational cost. That is called the [**kernel trick**](https://en.wikipedia.org/w/index.php?title=Kernel_method&oldid=747269865#Mathematics:_the_kernel_trick). For a kernel function to be *valid*, i.e. usable with the kernel trick, it should satisfy [two key properties](https://stats.stackexchange.com/q/48506/12359). There exist [many kernel functions to choose from](https://stats.stackexchange.com/q/18030/12359). As a side note, the kernel trick [may be applied to other machine learning models](https://stats.stackexchange.com/q/2167/12359), in which case they are referred as [*kernelized*](https://scholar.google.com/scholar?q=kernelized+&hl=en&as_sdt=0,22).\n",
    "\n",
    "## Going further  ##\n",
    "\n",
    "Some interesting QAs on SVMs:\n",
    "\n",
    "- [Best way to perform multiclass SVM](https://stats.stackexchange.com/q/21465/12359)\n",
    "- [Support vector machines and regression](https://stats.stackexchange.com/q/13194/12359)\n",
    "- [Understanding the different formulations for SVM](https://stats.stackexchange.com/q/157827/12359)\n",
    "- [What's the difference between $\\ell_1$-SVM, $\\ell_2$-SVM and LS-SVM loss functions?](https://stats.stackexchange.com/q/255486/12359)\n",
    "- To deal with an unbalanced dataset:\n",
    " - [Best way to handle unbalanced multiclass dataset with SVM](https://stats.stackexchange.com/q/20948/12359)\n",
    " - [A priori selection of SVM class weights](https://stats.stackexchange.com/a/24969/12359)\n",
    "- [How does one interpret SVM feature weights?](https://stats.stackexchange.com/q/39243/12359)\n",
    "- [Interpretating the C value in a linear SVM](https://stats.stackexchange.com/q/108639/12359)\n",
    "- [Generalization bounds on SVM](https://stats.stackexchange.com/q/259788/12359)\n",
    "- [General formula for the VC Dimension of a SVM](https://stats.stackexchange.com/q/255301/12359)\n",
    "- [What does the \"machine\" in \"support vector machine\" and \"restricted Boltzmann machine\" mean?](https://stats.stackexchange.com/q/261041/12359)\n",
    "- [How are SVMs = Template Matching?](https://stats.stackexchange.com/q/263587/12359)\n",
    "- [Single layer NeuralNetwork with ReLU activation equal to SVM?](https://stats.stackexchange.com/q/190883/12359)\n",
    "- [Comparing SVM and logistic regression](https://stats.stackexchange.com/q/95340/12359)\n",
    "\n",
    "Other links:\n",
    "\n",
    "-  [Least squares support vector machine](https://en.wikipedia.org/wiki/Least_squares_support_vector_machine)\n",
    "\n",
    "----------\n",
    "References:\n",
    "\n",
    "- {1} Chapelle, Olivier. \"Training a support vector machine in the primal.\" Neural computation 19, no. 5 (2007): 1155-1178. https://scholar.google.com/scholar?cluster=469291847682573606&hl=en&as_sdt=0,22 ; http://www.chapelle.cc/olivier/pub/neco07.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REFERENCES:\n",
    "- [In depth SVM](https://jakevdp.github.io/PythonDataScienceHandbook/05.07-support-vector-machines.html)\n",
    "- [SVM example](https://towardsdatascience.com/support-vector-machine-python-example-d67d9b63f1c8)\n",
    "- [SVM python](https://towardsdatascience.com/support-vector-machine-python-example-d67d9b63f1c8)\n",
    "- [Support Vector Machine: Python implementation using CVXOPT](https://xavierbourretsicotte.github.io/SVM_implementation.html)\n",
    "- [How does a Support Vector Machine (SVM) work?](https://stats.stackexchange.com/questions/23391/how-does-a-support-vector-machine-svm-work/353605#353605)\n",
    "- [Help me understand Support Vector Machines](https://stats.stackexchange.com/questions/3947/help-me-understand-support-vector-machines)\n",
    "- [MIT Prof. Patrick Winston](https://www.youtube.com/watch?v=_PwhiWxHK8o&feature=youtu.be)\n",
    "- [How can SVM 'find' an infinite feature space where linear separation is always possible?](https://stats.stackexchange.com/questions/80398/how-can-svm-find-an-infinite-feature-space-where-linear-separation-is-always-p/168309#168309)\n",
    "- [Khan academy Lagrange multiplier](https://www.youtube.com/watch?v=hQ4UNu1P2kw)\n",
    "- [Support Vector Machine — Using Numpy](https://medium.com/@saishruthi.tn/support-vector-machine-using-numpy-846f83f4183d)\n",
    "- [Mathematics behind SVM(Support Vector Machine)](https://towardsdatascience.com/mathematics-behind-svm-support-vector-machines-84742ddda0ca)\n",
    "- [Math behind SVM (Support Vector Machine)](https://medium.com/@ankitnitjsr13/math-behind-support-vector-machine-svm-5e7376d0ee4d)\n",
    "- [SVM From Scratch — Python](https://towardsdatascience.com/svm-implementation-from-scratch-python-2db2fc52e5c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
